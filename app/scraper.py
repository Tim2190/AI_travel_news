import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import logging
import urllib3
import re
import time
from typing import List, Dict, Optional, Tuple

# Playwright ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è gov.kz (–ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤)
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

# –û—Ç–∫–ª—é—á–∞–µ–º –Ω–∞–¥–æ–µ–¥–ª–∏–≤—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

# –ò–°–¢–û–ß–ù–ò–ö–ò: –û–§–ò–¶–ò–ê–õ–¨–ù–´–ï –°–ê–ô–¢–´ –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–• –û–†–ì–ê–ù–û–í (–†–£–°–°–ö–ò–ï –í–ï–†–°–ò–ò)
DIRECT_SCRAPE_SOURCES: List[Dict] = [
    # --- –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–ê (GOV.KZ - SPA, –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥) ---
    {
        "name": "–ú–∏–Ω–ù–∞—Ü–≠–∫–æ–Ω–æ–º–∏–∫–∏",
        "url": "https://www.gov.kz/memleket/entities/economy/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "economy",
    },
    {
        "name": "–ú–∏–Ω–§–∏–Ω",
        "url": "https://www.gov.kz/memleket/entities/minfin/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "minfin",
    },
    {
        "name": "–ú–ò–î –†–ö",
        "url": "https://www.gov.kz/memleket/entities/mfa/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mfa",
    },
    {
        "name": "–ú–í–î –†–ö",
        "url": "https://www.gov.kz/memleket/entities/qriim/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "qriim",
    },
    {
        "name": "–ú–∏–Ω–¢—Ä—É–¥–∞",
        "url": "https://www.gov.kz/memleket/entities/enbek/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "enbek",
    },
    {
        "name": "–ú–∏–Ω–ó–¥—Ä–∞–≤",
        "url": "https://www.gov.kz/memleket/entities/dsm/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "dsm",
    },
    {
        "name": "–ú–∏–Ω–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è",
        "url": "https://www.gov.kz/memleket/entities/edu/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "edu",
    },
    {
        "name": "–ú–∏–Ω–ù–∞—É–∫–∏",
        "url": "https://www.gov.kz/memleket/entities/sci/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "sci",
    },
    {
        "name": "–ú–∏–Ω–ü—Ä–æ–º–°—Ç—Ä–æ–π",
        "url": "https://www.gov.kz/memleket/entities/mps/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mps",
    },
    {
        "name": "–ú–∏–Ω–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç",
        "url": "https://www.gov.kz/memleket/entities/transport/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "transport",
    },
    {
        "name": "–ú–∏–Ω–¶–∏—Ñ—Ä—ã",
        "url": "https://www.gov.kz/memleket/entities/mdai/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mdai",
    },
    {
        "name": "–ú–∏–Ω–ö—É–ª—å—Ç—É—Ä—ã",
        "url": "https://www.gov.kz/memleket/entities/mam/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mam",
    },
    {
        "name": "–ú–∏–Ω–¢—É—Ä–∏–∑–º",
        "url": "https://www.gov.kz/memleket/entities/tsm/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "tsm",
    },
    {
        "name": "–ú–∏–Ω–≠–∫–æ–ª–æ–≥–∏–∏",
        "url": "https://www.gov.kz/memleket/entities/ecogeo/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "ecogeo",
    },
    {
        "name": "–ú–∏–Ω–°–µ–ª—å–•–æ–∑",
        "url": "https://www.gov.kz/memleket/entities/moa/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "moa",
    },
    {
        "name": "–ú–∏–Ω–≠–Ω–µ—Ä–≥–æ",
        "url": "https://www.gov.kz/memleket/entities/energo/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "energo",
    },
    {
        "name": "–ú–∏–Ω–Æ—Å—Ç",
        "url": "https://www.gov.kz/memleket/entities/adilet/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "adilet",
    },
    {
        "name": "–ú–ß–° –†–ö",
        "url": "https://www.gov.kz/memleket/entities/emer/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "emer",
    },
    {
        "name": "–ú–∏–Ω–¢–æ—Ä–≥–æ–≤–ª–∏",
        "url": "https://www.gov.kz/memleket/entities/mti/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mti",
    },

    # --- –ê–ö–ò–ú–ê–¢–´ –ú–ï–ì–ê–ü–û–õ–ò–°–û–í ---
    {
        "name": "–ê–∫–∏–º–∞—Ç –ê–ª–º–∞—Ç—ã",
        "url": "https://www.gov.kz/memleket/entities/almaty/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "almaty",
    },
    {
        "name": "–ê–∫–∏–º–∞—Ç –ê—Å—Ç–∞–Ω—ã",
        "url": "https://www.gov.kz/memleket/entities/astana/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "astana",
    },
]


async def _fetch_gov_kz_tokens() -> Optional[Dict]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç Playwright –û–î–ò–ù –†–ê–ó, –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç hash+token,
    –∫–æ—Ç–æ—Ä—ã–µ –±—Ä–∞—É–∑–µ—Ä –ø–µ—Ä–µ–¥–∞—ë—Ç –≤ API gov.kz.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è requests.
    """
    if not PLAYWRIGHT_AVAILABLE:
        logger.error("Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤—å –≤ requirements.txt: playwright")
        return None

    tokens = {}
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=["--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage"]
            )
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="ru-RU",
            )
            page = await context.new_page()

            def handle_request(request):
                if "api/v1/public/content-manager/news" in request.url:
                    h = request.headers
                    if h.get("hash") and h.get("token"):
                        tokens["hash"] = h["hash"]
                        tokens["token"] = h["token"]
                        tokens["referer"] = h.get("referer", "https://www.gov.kz/")
                        tokens["user-agent"] = h.get("user-agent", "")
                        tokens["sec-fetch-dest"] = h.get("sec-fetch-dest", "empty")
                        tokens["sec-fetch-mode"] = h.get("sec-fetch-mode", "cors")
                        tokens["sec-fetch-site"] = h.get("sec-fetch-site", "same-origin")
                        tokens["obtained_at"] = time.time()
                        logger.info("‚úÖ gov.kz —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–µ–Ω—ã —á–µ—Ä–µ–∑ Playwright")

            page.on("request", handle_request)

            await page.goto(
                "https://www.gov.kz/memleket/entities/economy/press/news?lang=ru",
                wait_until="domcontentloaded",
                timeout=60000,
            )
            
            await page.wait_for_selector(
                "a[href*='/press/news/details/']",
                timeout=45000,
            )
            await browser.close()

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ gov.kz: {e}")
        return None

    return tokens if tokens else None


class NewsScraper:
    def __init__(self, direct_sources: List[Dict] = None):
        self.direct_sources = direct_sources or DIRECT_SCRAPE_SOURCES

    # ========== ASYNC –ú–ï–¢–û–î –î–õ–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –° FASTAPI ==========
    async def scrape_async(self) -> List[Dict]:
        """
        Async-–≤–µ—Ä—Å–∏—è scrape() –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å FastAPI.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ë–ï–ó full_text –∏ –¥–∞—Ç—ã.
        –≠—Ç–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ —á–µ—Ä–µ–∑ enrich_news_with_content().
        """
        all_news = []
        gov_sources = [s for s in self.direct_sources if s.get("gov_kz")]

        if gov_sources:
            gov_news = await self._scrape_all_gov_kz_batched(gov_sources)
            all_news.extend(gov_news)

        logger.info(f"üìä –°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π (–±–µ–∑ full_text): {len(all_news)}")
        return all_news

    async def _scrape_all_gov_kz_batched(self, sources: List[Dict]) -> List[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –±–∞—Ç—á–∞–º–∏ –ø–æ 5 —à—Ç—É–∫.
        –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ–ª—É—á–∞—é—Ç—Å—è –°–í–ï–ñ–ò–ï —Ç–æ–∫–µ–Ω—ã —á–µ—Ä–µ–∑ Playwright.
        """
        all_news = []
        batch_size = 5
        
        total_batches = (len(sources) + batch_size - 1) // batch_size
        logger.info(f"üì¶ –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ {total_batches} –±–∞—Ç—á–µ–π –ø–æ {batch_size}")

        for batch_idx in range(0, len(sources), batch_size):
            batch = sources[batch_idx:batch_idx + batch_size]
            batch_num = batch_idx // batch_size + 1
            
            logger.info(f"üîÑ –ë–∞—Ç—á {batch_num}/{total_batches}: {[s['name'] for s in batch]}")
            
            try:
                tokens = await _fetch_gov_kz_tokens()
                if not tokens:
                    logger.error(f"‚ùå –ë–∞—Ç—á {batch_num}: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                age = time.time() - tokens.get('obtained_at', 0)
                logger.info(f"üîë –ë–∞—Ç—á {batch_num}: —Ç–æ–∫–µ–Ω—ã —Å–≤–µ–∂–∏–µ ({age:.1f} —Å–µ–∫)")
                
            except Exception as e:
                logger.error(f"‚ùå –ë–∞—Ç—á {batch_num}: –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤: {e}")
                continue

            for source in batch:
                try:
                    news = self._scrape_gov_kz_source(source, tokens)
                    all_news.extend(news)
                    time.sleep(0.7)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {source['name']}: {e}")
                    continue

            if batch_num < total_batches:
                logger.info(f"‚è∏Ô∏è  –ü–∞—É–∑–∞ 3 —Å–µ–∫ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –±–∞—Ç—á–µ–º...")
                time.sleep(3)

        logger.info(f"‚úÖ –í—Å–µ –±–∞—Ç—á–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)}")
        return all_news

    def _scrape_gov_kz_source(self, config: Dict, tokens: Dict) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç –¢–û–õ–¨–ö–û –¢–û–ü-3 –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —á–µ—Ä–µ–∑ API.
        –ë–ï–ó –ø–∞—Ä—Å–∏–Ω–≥–∞ full_text, image, date ‚Äî —ç—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ –ø–æ–∑–∂–µ.
        """
        name = config.get("name", "Unknown")
        project = config.get("project")
        base_url = config.get("base_url", "https://www.gov.kz")

        if not project:
            logger.warning(f"'{name}' –ø—Ä–æ–ø—É—â–µ–Ω: –Ω–µ —É–∫–∞–∑–∞–Ω 'project'")
            return []

        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º 20, –Ω–æ –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-3
        api_url = (
            f"https://www.gov.kz/api/v1/public/content-manager/news"
            f"?sort-by=created_date:DESC&projects=eq:{project}&page=1&size=20"
        )

        headers = {
            "accept": "application/json",
            "accept-language": "ru",
            "user-agent": tokens.get("user-agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"),
            "referer": f"{base_url}/memleket/entities/{project}/press/news?lang=ru",
            "hash": tokens["hash"],
            "token": tokens["token"],
            "sec-fetch-dest": tokens.get("sec-fetch-dest", "empty"),
            "sec-fetch-mode": tokens.get("sec-fetch-mode", "cors"),
            "sec-fetch-site": tokens.get("sec-fetch-site", "same-origin"),
            "origin": base_url,
        }

        news = []
        try:
            logger.info(f"API –∑–∞–ø—Ä–æ—Å: {name}...")
            resp = requests.get(api_url, headers=headers, timeout=15, verify=False)
            
            if resp.status_code != 200:
                logger.error(f"API {name} –≤–µ—Ä–Ω—É–ª –∫–æ–¥ {resp.status_code}")
                return []
            
            data = resp.json()

            items = []
            if isinstance(data, list):
                items = data
            elif isinstance(data, dict):
                items = data.get("content", []) or data.get("data", []) or data.get("items", [])

            if not items:
                logger.warning(f"{name}: API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫")
                return []

            # –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –¢–û–ü-3
            items = items[:3]
            logger.info(f"{name}: –ë–µ—Ä—ë–º —Ç–æ–ø-3 –∏–∑ {len(data) if isinstance(data, list) else len(data.get('content', []))}")

            for item in items:
                if not isinstance(item, dict):
                    continue

                title = item.get("name", "").strip() or item.get("title", "").strip()
                slug = item.get("id") or item.get("slug", "")
                
                if not title or not slug:
                    continue

                link = f"{base_url}/memleket/entities/{project}/press/news/details/{slug}?lang=ru"

                # –í–ê–ñ–ù–û: –ù–ï –ø–∞—Ä—Å–∏–º full_text, image, date
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                news.append({
                    "title": title,
                    "source_name": name,
                    "source_url": link,
                    # –≠—Ç–∏ –ø–æ–ª—è –±—É–¥—É—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –ø–æ–∑–∂–µ —á–µ—Ä–µ–∑ enrich_news_with_content()
                    "original_text": None,
                    "image_url": None,
                    "published_at": None,
                })

            logger.info(f"‚úÖ {name}: —Å–æ–±—Ä–∞–Ω–æ {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π (—Ç–æ–ø-3)")

        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ API {name}: {e}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ API {name}: {e}", exc_info=True)

        return news

    # ========== –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–ë–û–ì–ê–©–ï–ù–ò–ï –î–ê–ù–ù–´–ú–ò ==========
    def enrich_news_with_content(self, news_item: Dict) -> Dict:
        """
        –î–ª—è –û–î–ù–û–ô –Ω–æ–≤–æ—Å—Ç–∏ (–∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É –ë–î):
        1. –ü–∞—Ä—Å–∏—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∫–∞—Ä—Ç–∏–Ω–∫—É —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        2. –ò—â–µ—Ç –¥–∞—Ç—É –≤ –≤–∏–¥–∏–º–æ–º —Ç–µ–∫—Å—Ç–µ —á–µ—Ä–µ–∑ regex
        3. –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç datetime.now()
        
        –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –ü–û–°–õ–ï –ø—Ä–æ–≤–µ—Ä–∫–∏ "–µ—Å—Ç—å –ª–∏ title –≤ –ë–î".
        """
        url = news_item.get("source_url")
        if not url:
            logger.error("enrich_news_with_content: –Ω–µ—Ç source_url")
            return news_item

        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            
            if response.status_code != 200:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url} (–∫–æ–¥ {response.status_code})")
                # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
                news_item["published_at"] = datetime.now()
                return news_item
                
            soup = BeautifulSoup(response.content, "html.parser")

            # 1. –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
            paragraphs = soup.find_all("p")
            full_text = "\n".join([p.get_text() for p in paragraphs if len(p.get_text()) > 50])
            news_item["original_text"] = full_text if full_text else news_item["title"]

            # 2. –ò—â–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É
            image_url = None
            og = soup.find("meta", property="og:image")
            if og and og.get("content"):
                image_url = og.get("content")
            if not image_url:
                img = soup.find("img")
                if img and img.get("src"):
                    image_url = img.get("src")
            news_item["image_url"] = image_url

            # 3. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –∏—â–µ–º –¥–∞—Ç—É –≤ –í–ò–î–ò–ú–û–ú –¢–ï–ö–°–¢–ï
            page_text = soup.get_text()
            published_at = self._extract_date_from_text(page_text)
            
            if published_at:
                logger.info(f"‚úÖ –î–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ: {published_at.strftime('%Y-%m-%d')} –¥–ª—è [{news_item['title'][:50]}...]")
            else:
                # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é
                published_at = datetime.now()
                logger.warning(f"‚ö†Ô∏è –î–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–ª—è [{news_item['title'][:50]}...]")
            
            news_item["published_at"] = published_at

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è {url}: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
            news_item["published_at"] = datetime.now()

        return news_item

    # ========== –ü–ê–†–°–ò–ù–ì –î–ê–¢ –ò–ó –¢–ï–ö–°–¢–ê ==========
    def _extract_date_from_text(self, text: str) -> Optional[datetime]:
        """
        –ò—â–µ—Ç –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã:
        - "18 —Ñ–µ–≤—Ä–∞–ª—è 2025"
        - "18.02.2025"
        - "2025-02-18"
        """
        months_ru = {
            "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4,
            "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8,
            "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12
        }

        # 1. –§–æ—Ä–º–∞—Ç "18 —Ñ–µ–≤—Ä–∞–ª—è 2025"
        pattern1 = r"(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})"
        match = re.search(pattern1, text, re.IGNORECASE)
        if match:
            day = int(match.group(1))
            month = months_ru[match.group(2).lower()]
            year = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        # 2. –§–æ—Ä–º–∞—Ç "18.02.2025" –∏–ª–∏ "18/02/2025"
        pattern2 = r"(\d{1,2})[./](\d{1,2})[./](\d{4})"
        match = re.search(pattern2, text)
        if match:
            day = int(match.group(1))
            month = int(match.group(2))
            year = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        # 3. –§–æ—Ä–º–∞—Ç ISO "2025-02-18"
        pattern3 = r"(\d{4})-(\d{1,2})-(\d{1,2})"
        match = re.search(pattern3, text)
        if match:
            year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        return None


scraper = NewsScraper()
