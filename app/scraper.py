import logging
import requests
import re
import urllib3
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from .config import settings

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞—Ç
MONTHS_RU = {
    "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4, "–º–∞—è": 5, "–∏—é–Ω—è": 6,
    "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8, "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12,
    "—è–Ω–≤": 1, "—Ñ–µ–≤": 2, "–º–∞—Ä": 3, "–∞–ø—Ä": 4, "–º–∞–π": 5, "–∏—é–Ω": 6,
    "–∏—é–ª": 7, "–∞–≤–≥": 8, "—Å–µ–Ω": 9, "–æ–∫—Ç": 10, "–Ω–æ—è": 11, "–¥–µ–∫": 12
}

MONTHS_KZ = {
    "“õ–∞“£—Ç–∞—Ä": 1, "–∞“õ–ø–∞–Ω": 2, "–Ω–∞—É—Ä—ã–∑": 3, "—Å”ô—É—ñ—Ä": 4, "–º–∞–º—ã—Ä": 5, "–º–∞—É—Å—ã–º": 6,
    "—à—ñ–ª–¥–µ": 7, "—Ç–∞–º—ã–∑": 8, "“õ—ã—Ä–∫“Ø–π–µ–∫": 9, "“õ–∞–∑–∞–Ω": 10, "“õ–∞—Ä–∞—à–∞": 11, "–∂–µ–ª—Ç–æ“õ—Å–∞–Ω": 12,
    "“õ–∞“£": 1, "–∞“õ–ø": 2, "–Ω–∞—É": 3, "—Å”ô—É": 4, "–º–∞–º": 5, "–º–∞—É": 6,
    "—à—ñ–ª": 7, "—Ç–∞–º": 8, "“õ—ã—Ä": 9, "“õ–∞–∑": 10, "“õ–∞—Ä": 11, "–∂–µ–ª": 12
}

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∏ User-Agent
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7"
}

DIRECT_SOURCES = [
    {
        "name": "Akorda (–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç)",
        "url": "https://www.akorda.kz/ru/events",
        # –ò—â–µ–º –ª—é–±–æ–π –±–ª–æ–∫, –ø–æ—Ö–æ–∂–∏–π –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å
        "article_selector": ".event-item, .news-list__item, div.item, .news-item", 
        "title_selector": "h3, h4, .title",
        "link_selector": "a", # –õ—é–±–∞—è —Å—Å—ã–ª–∫–∞ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞
        "base_url": "https://www.akorda.kz"
    },
    {
        "name": "PrimeMinister (–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ)",
        "url": "https://primeminister.kz/ru/news",
        "article_selector": ".news_item, .card, .post-item, .news-list-item",
        "title_selector": ".news_title, .card-title, h3",
        "link_selector": "a",
        "base_url": "https://primeminister.kz"
    }
]

GOV_KZ_PROJECTS = {
    "–ú–∏–Ω–ù–∞—Ü–≠–∫–æ–Ω–æ–º–∏–∫–∏": 4, "–ú–∏–Ω–§–∏–Ω": 2, "–ú–ò–î –†–ö": 6, "–ú–í–î –†–ö": 11,
    "–ú–∏–Ω–¢—Ä—É–¥–∞": 21, "–ú–∏–Ω–ó–¥—Ä–∞–≤": 17, "–ú–∏–Ω–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è": 14, "–ú–∏–Ω–ù–∞—É–∫–∏": 15,
    "–ú–∏–Ω–ü—Ä–æ–º–°—Ç—Ä–æ–π": 3, "–ú–∏–Ω–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç": 22, "–ú–∏–Ω–¶–∏—Ñ—Ä—ã": 8, "–ú–∏–Ω–ö—É–ª—å—Ç—É—Ä—ã": 19,
    "–ú–∏–Ω–¢—É—Ä–∏–∑–º": 24, "–ú–∏–Ω–≠–∫–æ–ª–æ–≥–∏–∏": 16, "–ú–∏–Ω–°–µ–ª—å–•–æ–∑": 18, "–ú–∏–Ω–≠–Ω–µ—Ä–≥–æ": 20,
    "–ú–∏–Ω–Æ—Å—Ç": 9, "–ú–ß–° –†–ö": 5, "–ú–∏–Ω–¢–æ—Ä–≥–æ–≤–ª–∏": 23, "–ê–∫–∏–º–∞—Ç –ê–ª–º–∞—Ç—ã": 118, "–ê–∫–∏–º–∞—Ç –ê—Å—Ç–∞–Ω—ã": 105
}

class Scraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def parse_date(self, date_str: str) -> Optional[datetime]:
        if not date_str: return None
        date_str = str(date_str).strip().lower()

        # ISO
        try:
            iso_clean = date_str.split("+")[0].split(".")[0].replace("z", "")
            if "t" in iso_clean: return datetime.fromisoformat(iso_clean)
            if len(iso_clean) == 10 and "-" in iso_clean: return datetime.strptime(iso_clean, "%Y-%m-%d")
        except: pass

        # Text
        clean_text = re.sub(r"\s+\d{1,2}:\d{2}.*", "", date_str) 
        clean_text = re.sub(r"[^\w\s\.]", "", clean_text)
        
        if "." in clean_text:
            try: return datetime.strptime(clean_text, "%d.%m.%Y")
            except: pass

        parts = clean_text.split()
        if len(parts) >= 2:
            try:
                day = int(re.sub(r"\D", "", parts[0]))
                month_str = parts[1]
                month = MONTHS_RU.get(month_str) or MONTHS_KZ.get(month_str)
                year = datetime.now().year
                if len(parts) > 2 and parts[2].isdigit():
                    year = int(parts[2])
                    if 2020 < year < 2030: year = year
                if month: return datetime(year, month, day)
            except: pass
        return None

    def find_date_in_text(self, text: str) -> Optional[datetime]:
        if not text: return None
        head = text[:500]
        
        match_dots = re.search(r"\d{2}\.\d{2}\.\d{4}", head)
        if match_dots: return self.parse_date(match_dots.group(0))

        match_text = re.search(r"\d{1,2}\s+[–∞-—è–ê-–Ø”ô—ñ“£“ì“Ø“±“õ”©“ª]{3,}\s+\d{4}", head)
        if match_text: return self.parse_date(match_text.group(0))
        return None

    def scrape(self) -> List[Dict]:
        logger.info("üîç START SCRAPING...")
        all_news = []
        all_news.extend(self.scrape_gov_kz_api())
        for source in DIRECT_SOURCES:
            all_news.extend(self.scrape_direct(source))
        
        logger.info(f"‚úÖ SCRAPE FINISHED. Found: {len(all_news)}")
        return all_news

    def scrape_gov_kz_api(self) -> List[Dict]:
        results = []
        base_api = "https://gov.kz/api/v1/public/news"
        
        for name, project_id in GOV_KZ_PROJECTS.items():
            try:
                params = {"projects": project_id, "lang": "ru", "limit": 3}
                # –î–æ–±–∞–≤–∏–ª —Ç–∞–π–º–∞—É—Ç –ø–æ–±–æ–ª—å—à–µ
                resp = self.session.get(base_api, params=params, timeout=15, verify=False)
                
                if resp.status_code == 200:
                    data = resp.json()
                    items = data if isinstance(data, list) else data.get("content", [])
                    
                    for item in items:
                        title = item.get("title")
                        if not title: continue

                        pub_date = self.parse_date(item.get("publish_date") or item.get("created_date"))
                        body = item.get("body") or ""
                        soup = BeautifulSoup(body, "html.parser")
                        text = soup.get_text(separator="\n").strip()

                        if not pub_date: pub_date = self.find_date_in_text(text)
                        if not pub_date: pub_date = datetime.now()

                        news_id = item.get("id")
                        proj_id_from_api = item.get("projects", [project_id])[0]
                        link = f"https://gov.kz/memleket/entities/{proj_id_from_api}/press/news/details/{news_id}?lang=ru"
                        
                        img = None
                        if item.get("visual_content"):
                             img = item["visual_content"][0].get("source")

                        results.append({
                            "title": title, "original_text": text[:4000],
                            "source_name": name, "source_url": link,
                            "published_at": pub_date, "image_url": img
                        })
                    # –£–±—Ä–∞–ª —Å–ø–∞–º "API OK", —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω–æ –Ω–∞—à–ª–∏
                    if items: logger.info(f"API {name}: found {len(items)}")
                else:
                    # –í–ê–ñ–ù–û: –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–¥ –æ—à–∏–±–∫–∏
                    logger.warning(f"API {name} Failed: Status {resp.status_code}")
            except Exception as e:
                logger.error(f"API {name} Error: {e}")
        return results

    def scrape_direct(self, config: Dict) -> List[Dict]:
        results = []
        name = config["name"]
        try:
            resp = self.session.get(config["url"], timeout=20, verify=False)
            if resp.status_code != 200: 
                logger.warning(f"Direct {name}: Status {resp.status_code}")
                return []
            
            soup = BeautifulSoup(resp.content, "html.parser")
            items = soup.select(config["article_selector"])[:3]
            
            if not items:
                logger.warning(f"Direct {name}: No items found (check selectors)")

            for item in items:
                # –ò—â–µ–º –ª—é–±—É—é —Å—Å—ã–ª–∫—É –≤ –±–ª–æ–∫–µ
                link_el = item.select_one(config["link_selector"])
                if not link_el: continue
                
                href = link_el.get("href")
                if not href: continue
                full_link = config["base_url"] + href if href.startswith("/") else href
                
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –ª–∏–±–æ –∏–∑ —Ç–µ–≥–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –ª–∏–±–æ —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
                title_el = item.select_one(config["title_selector"])
                title = title_el.get_text(strip=True) if title_el else link_el.get_text(strip=True)

                full_text, image, pub_date = self.fetch_details(full_link)
                
                if not pub_date:
                    date_el = item.find(string=re.compile(r"\d{1,2}\s+[–∞-—è–ê-–Ø]{3,}\s+\d{4}"))
                    if date_el: pub_date = self.parse_date(date_el)
                
                if not pub_date: pub_date = datetime.now()
                
                results.append({
                    "title": title, "original_text": full_text or title,
                    "source_name": name, "source_url": full_link,
                    "published_at": pub_date, "image_url": image
                })
            
            if results: logger.info(f"Direct {name}: found {len(results)}")
        except Exception as e:
            logger.error(f"Direct {name} Error: {e}")
        return results

    def fetch_details(self, url: str):
        try:
            resp = self.session.get(url, timeout=10, verify=False)
            soup = BeautifulSoup(resp.content, "html.parser")
            text = soup.get_text(separator="\n").strip()
            
            img = None
            meta_img = soup.find("meta", property="og:image")
            if meta_img: img = meta_img.get("content")

            pub_date = None
            for prop in ["article:published_time", "published_time", "date"]:
                meta = soup.find("meta", property=prop) or soup.find("meta", attrs={"name": prop})
                if meta:
                    pub_date = self.parse_date(meta.get("content"))
                    if pub_date: break
            
            if not pub_date: pub_date = self.find_date_in_text(text)

            return text, img, pub_date
        except:
            return None, None, None

scraper = Scraper()
