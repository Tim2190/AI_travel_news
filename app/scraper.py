import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import logging
import urllib3
import re
import time
from typing import List, Dict, Optional, Tuple

# Playwright ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è gov.kz (–ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤)
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

# –û—Ç–∫–ª—é—á–∞–µ–º –Ω–∞–¥–æ–µ–¥–ª–∏–≤—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

# –ò–°–¢–û–ß–ù–ò–ö–ò: –û–§–ò–¶–ò–ê–õ–¨–ù–´–ï –°–ê–ô–¢–´ –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–• –û–†–ì–ê–ù–û–í (–†–£–°–°–ö–ò–ï –í–ï–†–°–ò–ò)
DIRECT_SCRAPE_SOURCES: List[Dict] = [
    # --- –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–ê (GOV.KZ - SPA, –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥) ---
    {
        "name": "–ú–∏–Ω–ù–∞—Ü–≠–∫–æ–Ω–æ–º–∏–∫–∏",
        "url": "https://www.gov.kz/memleket/entities/economy/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "economy",
    },
    {
        "name": "–ú–∏–Ω–§–∏–Ω",
        "url": "https://www.gov.kz/memleket/entities/minfin/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "minfin",
    },
    {
        "name": "–ú–ò–î –†–ö",
        "url": "https://www.gov.kz/memleket/entities/mfa/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mfa",
    },
    {
        "name": "–ú–í–î –†–ö",
        "url": "https://www.gov.kz/memleket/entities/qriim/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "qriim",
    },
    {
        "name": "–ú–∏–Ω–¢—Ä—É–¥–∞",
        "url": "https://www.gov.kz/memleket/entities/enbek/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "enbek",
    },
    {
        "name": "–ú–∏–Ω–ó–¥—Ä–∞–≤",
        "url": "https://www.gov.kz/memleket/entities/dsm/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "dsm",
    },
    {
        "name": "–ú–∏–Ω–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è",
        "url": "https://www.gov.kz/memleket/entities/edu/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "edu",
    },
    {
        "name": "–ú–∏–Ω–ù–∞—É–∫–∏",
        "url": "https://www.gov.kz/memleket/entities/sci/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "sci",
    },
    {
        "name": "–ú–∏–Ω–ü—Ä–æ–º–°—Ç—Ä–æ–π",
        "url": "https://www.gov.kz/memleket/entities/mps/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mps",
    },
    {
        "name": "–ú–∏–Ω–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç",
        "url": "https://www.gov.kz/memleket/entities/transport/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "transport",
    },
    {
        "name": "–ú–∏–Ω–¶–∏—Ñ—Ä—ã",
        "url": "https://www.gov.kz/memleket/entities/mdai/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mdai",
    },
    {
        "name": "–ú–∏–Ω–ö—É–ª—å—Ç—É—Ä—ã",
        "url": "https://www.gov.kz/memleket/entities/mam/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mam",
    },
    {
        "name": "–ú–∏–Ω–¢—É—Ä–∏–∑–º",
        "url": "https://www.gov.kz/memleket/entities/tsm/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "tsm",
    },
    {
        "name": "–ú–∏–Ω–≠–∫–æ–ª–æ–≥–∏–∏",
        "url": "https://www.gov.kz/memleket/entities/ecogeo/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "ecogeo",
    },
    {
        "name": "–ú–∏–Ω–°–µ–ª—å–•–æ–∑",
        "url": "https://www.gov.kz/memleket/entities/moa/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "moa",
    },
    {
        "name": "–ú–∏–Ω–≠–Ω–µ—Ä–≥–æ",
        "url": "https://www.gov.kz/memleket/entities/energo/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "energo",
    },
    {
        "name": "–ú–∏–Ω–Æ—Å—Ç",
        "url": "https://www.gov.kz/memleket/entities/adilet/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "adilet",
    },
    {
        "name": "–ú–ß–° –†–ö",
        "url": "https://www.gov.kz/memleket/entities/emer/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "emer",
    },
    {
        "name": "–ú–∏–Ω–¢–æ—Ä–≥–æ–≤–ª–∏",
        "url": "https://www.gov.kz/memleket/entities/mti/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mti",
    },

    # --- –ê–ö–ò–ú–ê–¢–´ –ú–ï–ì–ê–ü–û–õ–ò–°–û–í ---
    {
        "name": "–ê–∫–∏–º–∞—Ç –ê–ª–º–∞—Ç—ã",
        "url": "https://www.gov.kz/memleket/entities/almaty/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "almaty",
    },
    {
        "name": "–ê–∫–∏–º–∞—Ç –ê—Å—Ç–∞–Ω—ã",
        "url": "https://www.gov.kz/memleket/entities/astana/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "astana",
    },
]


async def _fetch_gov_kz_tokens() -> Optional[Dict]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç Playwright –û–î–ò–ù –†–ê–ó, –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç hash+token,
    –∫–æ—Ç–æ—Ä—ã–µ –±—Ä–∞—É–∑–µ—Ä –ø–µ—Ä–µ–¥–∞—ë—Ç –≤ API gov.kz.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è requests.
    """
    if not PLAYWRIGHT_AVAILABLE:
        logger.error("Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤—å –≤ requirements.txt: playwright")
        return None

    tokens = {}
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=["--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage"]
            )
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="ru-RU",
            )
            page = await context.new_page()

            def handle_request(request):
                if "api/v1/public/content-manager/news" in request.url:
                    h = request.headers
                    if h.get("hash") and h.get("token"):
                        tokens["hash"] = h["hash"]
                        tokens["token"] = h["token"]
                        tokens["referer"] = h.get("referer", "https://www.gov.kz/")
                        tokens["user-agent"] = h.get("user-agent", "")
                        tokens["sec-fetch-dest"] = h.get("sec-fetch-dest", "empty")
                        tokens["sec-fetch-mode"] = h.get("sec-fetch-mode", "cors")
                        tokens["sec-fetch-site"] = h.get("sec-fetch-site", "same-origin")
                        tokens["obtained_at"] = time.time()  # –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –≤—Ä–µ–º—è –ø–æ–ª—É—á–µ–Ω–∏—è
                        logger.info("‚úÖ gov.kz —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–µ–Ω—ã —á–µ—Ä–µ–∑ Playwright")

            page.on("request", handle_request)

            await page.goto(
                "https://www.gov.kz/memleket/entities/economy/press/news?lang=ru",
                wait_until="domcontentloaded",
                timeout=60000,
            )
            
            await page.wait_for_selector(
                "a[href*='/press/news/details/']",
                timeout=45000,
            )
            await browser.close()

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ gov.kz: {e}")
        return None

    return tokens if tokens else None


class NewsScraper:
    def __init__(self, direct_sources: List[Dict] = None):
        self.direct_sources = direct_sources or DIRECT_SCRAPE_SOURCES

    # ========== ASYNC –ú–ï–¢–û–î –î–õ–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –° FASTAPI ==========
    async def scrape_async(self) -> List[Dict]:
        """
        Async-–≤–µ—Ä—Å–∏—è scrape() –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å FastAPI.
        –í—ã–∑—ã–≤–∞–π –µ—ë –∏–∑ FastAPI —Ç–∞–∫: await scraper.scrape_async()
        """
        all_news = []

        # –¢–æ–ª—å–∫–æ gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (Akorda –∏ PrimeMinister —É–±—Ä–∞–ª–∏)
        gov_sources = [s for s in self.direct_sources if s.get("gov_kz")]

        # gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äî async –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥ —Å –±–∞—Ç—á–∞–º–∏
        if gov_sources:
            gov_news = await self._scrape_all_gov_kz_batched(gov_sources)
            all_news.extend(gov_news)

        logger.info(f"Total news gathered: {len(all_news)}")
        return all_news

    async def _scrape_all_gov_kz_batched(self, sources: List[Dict]) -> List[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –±–∞—Ç—á–∞–º–∏ –ø–æ 5 —à—Ç—É–∫.
        –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ–ª—É—á–∞—é—Ç—Å—è –°–í–ï–ñ–ò–ï —Ç–æ–∫–µ–Ω—ã —á–µ—Ä–µ–∑ Playwright.
        –≠—Ç–æ –∑–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ—Ç—É—Ö–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ rate limiting.
        """
        all_news = []
        batch_size = 5
        
        total_batches = (len(sources) + batch_size - 1) // batch_size
        logger.info(f"üì¶ –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ {total_batches} –±–∞—Ç—á–µ–π –ø–æ {batch_size}")

        for batch_idx in range(0, len(sources), batch_size):
            batch = sources[batch_idx:batch_idx + batch_size]
            batch_num = batch_idx // batch_size + 1
            
            logger.info(f"üîÑ –ë–∞—Ç—á {batch_num}/{total_batches}: {[s['name'] for s in batch]}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂–∏–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
            try:
                tokens = await _fetch_gov_kz_tokens()
                if not tokens:
                    logger.error(f"‚ùå –ë–∞—Ç—á {batch_num}: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                age = time.time() - tokens.get('obtained_at', 0)
                logger.info(f"üîë –ë–∞—Ç—á {batch_num}: —Ç–æ–∫–µ–Ω—ã —Å–≤–µ–∂–∏–µ ({age:.1f} —Å–µ–∫)")
                
            except Exception as e:
                logger.error(f"‚ùå –ë–∞—Ç—á {batch_num}: –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤: {e}")
                continue

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –±–∞—Ç—á–µ
            for source in batch:
                try:
                    news = self._scrape_gov_kz_source(source, tokens)
                    all_news.extend(news)
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                    time.sleep(0.7)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {source['name']}: {e}")
                    continue

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ (—á—Ç–æ–±—ã –Ω–µ –ø–∞–ª–∏—Ç—å—Å—è –ø–µ—Ä–µ–¥ —Å–µ—Ä–≤–µ—Ä–æ–º)
            if batch_num < total_batches:
                logger.info(f"‚è∏Ô∏è  –ü–∞—É–∑–∞ 3 —Å–µ–∫ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –±–∞—Ç—á–µ–º...")
                time.sleep(3)

        logger.info(f"‚úÖ –í—Å–µ –±–∞—Ç—á–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)}")
        return all_news

    def _scrape_gov_kz_source(self, config: Dict, tokens: Dict) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫ —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π API –∑–∞–ø—Ä–æ—Å —Å —Ç–æ–∫–µ–Ω–∞–º–∏.
        """
        name = config.get("name", "Unknown")
        project = config.get("project")
        base_url = config.get("base_url", "https://www.gov.kz")

        if not project:
            logger.warning(f"'{name}' –ø—Ä–æ–ø—É—â–µ–Ω: –Ω–µ —É–∫–∞–∑–∞–Ω 'project'")
            return []

        api_url = (
            f"https://www.gov.kz/api/v1/public/content-manager/news"
            f"?sort-by=created_date:DESC&projects=eq:{project}&page=1&size=20"
        )

        headers = {
            "accept": "application/json",
            "accept-language": "ru",
            "user-agent": tokens.get("user-agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"),
            "referer": f"{base_url}/memleket/entities/{project}/press/news?lang=ru",
            "hash": tokens["hash"],
            "token": tokens["token"],
            "sec-fetch-dest": tokens.get("sec-fetch-dest", "empty"),
            "sec-fetch-mode": tokens.get("sec-fetch-mode", "cors"),
            "sec-fetch-site": tokens.get("sec-fetch-site", "same-origin"),
            "origin": base_url,
        }

        news = []
        try:
            logger.info(f"API –∑–∞–ø—Ä–æ—Å: {name}...")
            resp = requests.get(api_url, headers=headers, timeout=15, verify=False)
            
            if resp.status_code != 200:
                logger.error(f"API {name} –≤–µ—Ä–Ω—É–ª –∫–æ–¥ {resp.status_code}")
                logger.error(f"–û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {resp.text[:500]}")
                return []
            
            data = resp.json()

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞
            items = []
            if isinstance(data, list):
                items = data
                logger.info(f"{name}: API –≤–µ—Ä–Ω—É–ª —Å–ø–∏—Å–æ–∫ –∏–∑ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            elif isinstance(data, dict):
                items = data.get("content", [])
                if not items:
                    items = data.get("data", []) or data.get("items", []) or data.get("news", [])
            else:
                logger.error(f"{name}: Unexpected API response type: {type(data)}")
                return []

            if not items:
                logger.warning(f"{name}: API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")
                return []

            logger.info(f"{name}: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π")

            for item in items:
                if not isinstance(item, dict):
                    continue

                title = item.get("name", "").strip() or item.get("title", "").strip()
                slug = item.get("id") or item.get("slug", "")
                
                if not title or not slug:
                    continue

                link = f"{base_url}/memleket/entities/{project}/press/news/details/{slug}?lang=ru"

                # –î–∞—Ç–∞ –∏–∑ API
                published_at = None
                raw_date = item.get("createdDate") or item.get("created_date") or item.get("publishedDate") or item.get("date")
                if raw_date:
                    published_at = self._parse_date(str(raw_date))

                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –î–ê–¢
                if published_at:
                    logger.info(f"  üìÖ [{title[:40]}...] ‚Üí –î–∞—Ç–∞ –∏–∑ API: {published_at.strftime('%Y-%m-%d %H:%M')}")
                else:
                    logger.warning(f"  ‚ö†Ô∏è [{title[:40]}...] ‚Üí –î–∞—Ç–∞ –≤ API –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")

                # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∫–∞—Ä—Ç–∏–Ω–∫—É –±–µ—Ä—ë–º —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏
                full_text, image_url, page_date = self._fetch_full_text_and_image(link)

                # –í–ê–ñ–ù–û: –µ—Å–ª–∏ –¥–∞—Ç–∞ –∏–∑ API –ø—É—Å—Ç–∞—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                final_date = published_at or page_date

                if final_date:
                    days_old = (datetime.now() - final_date).days
                    logger.info(f"  ‚úÖ –§–ò–ù–ê–õ–¨–ù–ê–Ø –î–ê–¢–ê: {final_date.strftime('%Y-%m-%d %H:%M')} (–≤–æ–∑—Ä–∞—Å—Ç: {days_old} –¥–Ω–µ–π)")
                else:
                    logger.error(f"  ‚ùå [{title[:40]}...] ‚Üí –î–ê–¢–ê –ù–ï –ù–ê–ô–î–ï–ù–ê –ù–ò–ì–î–ï!")

                news.append({
                    "title": title,
                    "original_text": full_text or title,
                    "source_name": name,
                    "source_url": link,
                    "image_url": image_url,
                    "published_at": final_date,
                })

            logger.info(f"‚úÖ {name}: —Å–æ–±—Ä–∞–Ω–æ {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ API {name}: {e}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ API {name}: {e}", exc_info=True)

        return news

    # ========== –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –î–ê–¢ ==========
    def _extract_publish_date(self, soup: BeautifulSoup) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤, <time> –∏ —Ç–µ–∫—Å—Ç–∞"""
        # 1. –ú–µ—Ç–∞-—Ç–µ–≥–∏
        for prop in ("article:published_time", "published_time", "date", "og:updated_time"):
            meta = soup.find("meta", property=prop) or soup.find("meta", attrs={"name": prop})
            if meta and meta.get("content"):
                parsed = self._parse_date(meta["content"])
                if parsed:
                    return parsed

        # 2. <time datetime="">
        time_el = soup.find("time", attrs={"datetime": True})
        if time_el and time_el.get("datetime"):
            parsed = self._parse_date(time_el["datetime"])
            if parsed:
                return parsed

        # 3. –¢–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        text = soup.get_text()
        date_from_text = self._extract_date_from_text(text)
        if date_from_text:
            return date_from_text

        return None

    def _extract_date_from_text(self, text: str) -> Optional[datetime]:
        """–ò—â–µ—Ç –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ —á–µ—Ä–µ–∑ regex"""
        months_ru = {
            "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4,
            "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8,
            "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12
        }

        # 1. "18 —Ñ–µ–≤—Ä–∞–ª—è 2025"
        pattern1 = r"(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})"
        match = re.search(pattern1, text, re.IGNORECASE)
        if match:
            day = int(match.group(1))
            month = months_ru[match.group(2).lower()]
            year = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        # 2. "18.02.2025"
        pattern2 = r"(\d{1,2})[./](\d{1,2})[./](\d{4})"
        match = re.search(pattern2, text)
        if match:
            day = int(match.group(1))
            month = int(match.group(2))
            year = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        # 3. ISO "2025-02-18"
        pattern3 = r"(\d{4})-(\d{1,2})-(\d{1,2})"
        match = re.search(pattern3, text)
        if match:
            year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        return None

    def _parse_date(self, value: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç ISO –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if not value or not value.strip():
            return None
        value = value.strip()[:25]
        for fmt in ("%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
            try:
                if value.endswith("Z"):
                    value = value[:-1] + "+00:00"
                if "+" in value or value.count("-") >= 2:
                    d = datetime.fromisoformat(value.replace("Z", "+00:00"))
                else:
                    d = datetime.strptime(value[:10], "%Y-%m-%d")
                if d.tzinfo:
                    d = d.astimezone(timezone.utc).replace(tzinfo=None)
                return d
            except Exception:
                continue
        return None

    def _fetch_full_text_and_image(self, url: str) -> Tuple[Optional[str], Optional[str], Optional[datetime]]:
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            if response.status_code != 200:
                return None, None, None
            soup = BeautifulSoup(response.content, "html.parser")

            paragraphs = soup.find_all("p")
            text = "\n".join([p.get_text() for p in paragraphs if len(p.get_text()) > 50])

            image_url = None
            og = soup.find("meta", property="og:image")
            if og and og.get("content"):
                image_url = og.get("content")
            if not image_url:
                img = soup.find("img")
                if img and img.get("src"):
                    image_url = img.get("src")

            published_at = self._extract_publish_date(soup)
            return text, image_url, published_at
        except Exception:
            return None, None, None


scraper = NewsScraper()
