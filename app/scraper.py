import logging
import requests
import re
import urllib3
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from .config import settings

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞—Ç
MONTHS_RU = {
    "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4, "–º–∞—è": 5, "–∏—é–Ω—è": 6,
    "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8, "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12,
    "—è–Ω–≤": 1, "—Ñ–µ–≤": 2, "–º–∞—Ä": 3, "–∞–ø—Ä": 4, "–º–∞–π": 5, "–∏—é–Ω": 6,
    "–∏—é–ª": 7, "–∞–≤–≥": 8, "—Å–µ–Ω": 9, "–æ–∫—Ç": 10, "–Ω–æ—è": 11, "–¥–µ–∫": 12
}

MONTHS_KZ = {
    "“õ–∞“£—Ç–∞—Ä": 1, "–∞“õ–ø–∞–Ω": 2, "–Ω–∞—É—Ä—ã–∑": 3, "—Å”ô—É—ñ—Ä": 4, "–º–∞–º—ã—Ä": 5, "–º–∞—É—Å—ã–º": 6,
    "—à—ñ–ª–¥–µ": 7, "—Ç–∞–º—ã–∑": 8, "“õ—ã—Ä–∫“Ø–π–µ–∫": 9, "“õ–∞–∑–∞–Ω": 10, "“õ–∞—Ä–∞—à–∞": 11, "–∂–µ–ª—Ç–æ“õ—Å–∞–Ω": 12,
    "“õ–∞“£": 1, "–∞“õ–ø": 2, "–Ω–∞—É": 3, "—Å”ô—É": 4, "–º–∞–º": 5, "–º–∞—É": 6,
    "—à—ñ–ª": 7, "—Ç–∞–º": 8, "“õ—ã—Ä": 9, "“õ–∞–∑": 10, "“õ–∞—Ä": 11, "–∂–µ–ª": 12
}

# 1. –ü—Ä—è–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
DIRECT_SOURCES = [
    {
        "name": "Akorda (–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç)",
        "url": "https://www.akorda.kz/ru/events",
        "article_selector": ".event-item, .news-list__item, div.item",
        "title_selector": "h3 a, .title a, a",
        "link_selector": "h3 a, .title a, a",
        "base_url": "https://www.akorda.kz"
    },
    {
        "name": "PrimeMinister (–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ)",
        "url": "https://primeminister.kz/ru/news",
        "article_selector": ".news_item, .card, .post-item",
        "title_selector": ".news_title a, .card-title a, a",
        "link_selector": "a",
        "base_url": "https://primeminister.kz"
    }
]

# 2. GOV.KZ (API ID)
GOV_KZ_PROJECTS = {
    "–ú–∏–Ω–ù–∞—Ü–≠–∫–æ–Ω–æ–º–∏–∫–∏": 4, "–ú–∏–Ω–§–∏–Ω": 2, "–ú–ò–î –†–ö": 6, "–ú–í–î –†–ö": 11,
    "–ú–∏–Ω–¢—Ä—É–¥–∞": 21, "–ú–∏–Ω–ó–¥—Ä–∞–≤": 17, "–ú–∏–Ω–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è": 14, "–ú–∏–Ω–ù–∞—É–∫–∏": 15,
    "–ú–∏–Ω–ü—Ä–æ–º–°—Ç—Ä–æ–π": 3, "–ú–∏–Ω–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç": 22, "–ú–∏–Ω–¶–∏—Ñ—Ä—ã": 8, "–ú–∏–Ω–ö—É–ª—å—Ç—É—Ä—ã": 19,
    "–ú–∏–Ω–¢—É—Ä–∏–∑–º": 24, "–ú–∏–Ω–≠–∫–æ–ª–æ–≥–∏–∏": 16, "–ú–∏–Ω–°–µ–ª—å–•–æ–∑": 18, "–ú–∏–Ω–≠–Ω–µ—Ä–≥–æ": 20,
    "–ú–∏–Ω–Æ—Å—Ç": 9, "–ú–ß–° –†–ö": 5, "–ú–∏–Ω–¢–æ—Ä–≥–æ–≤–ª–∏": 23, "–ê–∫–∏–º–∞—Ç –ê–ª–º–∞—Ç—ã": 118, "–ê–∫–∏–º–∞—Ç –ê—Å—Ç–∞–Ω—ã": 105
}

class Scraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0 Safari/537.36",
            "Accept": "application/json, text/html, */*"
        })

    def parse_date(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–µ—Ä –¥–∞—Ç—ã."""
        if not date_str: return None
        date_str = str(date_str).strip().lower()

        try:
            iso_clean = date_str.split("+")[0].split(".")[0].replace("z", "")
            if "t" in iso_clean: return datetime.fromisoformat(iso_clean)
            if len(iso_clean) == 10 and "-" in iso_clean: return datetime.strptime(iso_clean, "%Y-%m-%d")
        except: pass

        clean_text = re.sub(r"\s+\d{1,2}:\d{2}.*", "", date_str) 
        clean_text = re.sub(r"[^\w\s\.]", "", clean_text)
        
        if "." in clean_text:
            try: return datetime.strptime(clean_text, "%d.%m.%Y")
            except: pass

        parts = clean_text.split()
        if len(parts) >= 2:
            try:
                day = int(re.sub(r"\D", "", parts[0]))
                month_str = parts[1]
                month = MONTHS_RU.get(month_str) or MONTHS_KZ.get(month_str)
                year = datetime.now().year
                if len(parts) > 2 and parts[2].isdigit():
                    year = int(parts[2])
                    if 2020 < year < 2030: year = year
                if month: return datetime(year, month, day)
            except: pass
        return None

    def find_date_in_text(self, text: str) -> Optional[datetime]:
        """–ò—â–µ—Ç –¥–∞—Ç—É –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞."""
        if not text: return None
        head = text[:500] # –ë–µ—Ä–µ–º –ø–æ–±–æ–ª—å—à–µ, 500 —Å–∏–º–≤–æ–ª–æ–≤
        
        match_dots = re.search(r"\d{2}\.\d{2}\.\d{4}", head)
        if match_dots: return self.parse_date(match_dots.group(0))

        match_text = re.search(r"\d{1,2}\s+[–∞-—è–ê-–Ø”ô—ñ“£“ì“Ø“±“õ”©“ª]{3,}\s+\d{4}", head)
        if match_text: return self.parse_date(match_text.group(0))
        return None

    def scrape(self) -> List[Dict]:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞."""
        logger.warning("üèÅ STARTING FULL SCRAPE CYCLE...") # WARNING —á—Ç–æ–±—ã —Ç–æ—á–Ω–æ –±—ã–ª–æ –≤–∏–¥–Ω–æ –≤ –ª–æ–≥–∞—Ö
        all_news = []
        all_news.extend(self.scrape_gov_kz_api())
        for source in DIRECT_SOURCES:
            all_news.extend(self.scrape_direct(source))
        logger.warning(f"‚úÖ CYCLE FINISHED. Total items found: {len(all_news)}")
        return all_news

    def scrape_gov_kz_api(self) -> List[Dict]:
        results = []
        base_api = "https://gov.kz/api/v1/public/news"
        
        for name, project_id in GOV_KZ_PROJECTS.items():
            try:
                params = {"projects": project_id, "lang": "ru", "limit": 3}
                resp = self.session.get(base_api, params=params, timeout=10, verify=False)
                
                if resp.status_code == 200:
                    data = resp.json()
                    items = data if isinstance(data, list) else data.get("content", [])
                    
                    for item in items:
                        title = item.get("title")
                        if not title: continue

                        # 1. –î–∞—Ç–∞ –∏–∑ API
                        pub_date = self.parse_date(item.get("publish_date") or item.get("created_date"))

                        # 2. –¢–µ–∫—Å—Ç (–í–°–ï –¢–ï–ì–ò, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ p)
                        body = item.get("body") or ""
                        soup = BeautifulSoup(body, "html.parser")
                        text = soup.get_text(separator="\n").strip()

                        # 3. –î–∞—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ API –ø—É—Å—Ç–æ–µ)
                        if not pub_date:
                            pub_date = self.find_date_in_text(text)

                        # 4. FALLBACK: –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç –≤–æ–æ–±—â–µ –Ω–∏–≥–¥–µ ‚Äî –±–µ—Ä–µ–º –¢–ï–ö–£–©–£–Æ.
                        # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ —Ç–µ—Ä—è–ª–∏—Å—å.
                        if not pub_date:
                            pub_date = datetime.now()

                        news_id = item.get("id")
                        proj_id_from_api = item.get("projects", [project_id])[0]
                        link = f"https://gov.kz/memleket/entities/{proj_id_from_api}/press/news/details/{news_id}?lang=ru"
                        
                        img = None
                        if item.get("visual_content"):
                             img = item["visual_content"][0].get("source")

                        results.append({
                            "title": title,
                            "original_text": text[:4000],
                            "source_name": name,
                            "source_url": link,
                            "published_at": pub_date,
                            "image_url": img
                        })
                    logger.info(f"API {name}: OK ({len(items)})")
            except Exception as e:
                logger.error(f"API {name} Error: {e}")
        return results

    def scrape_direct(self, config: Dict) -> List[Dict]:
        results = []
        name = config["name"]
        try:
            resp = self.session.get(config["url"], timeout=20, verify=False)
            if resp.status_code != 200: 
                logger.warning(f"Direct {name}: Status {resp.status_code}")
                return []
            
            soup = BeautifulSoup(resp.content, "html.parser")
            items = soup.select(config["article_selector"])[:3]
            
            if not items:
                logger.warning(f"Direct {name}: No items found by selector")

            for item in items:
                link_el = item.select_one(config["link_selector"])
                if not link_el: continue
                href = link_el.get("href")
                if not href: continue
                full_link = config["base_url"] + href if href.startswith("/") else href
                title = link_el.get_text(strip=True)

                # –ó–∞—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å
                full_text, image, pub_date = self.fetch_details(full_link)
                
                # –ï—Å–ª–∏ fetch_details –Ω–µ –Ω–∞—à–µ–ª –¥–∞—Ç—É, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–Ω–∞—Ä—É–∂–∏
                if not pub_date:
                    date_el = item.find(string=re.compile(r"\d{1,2}\s+[–∞-—è–ê-–Ø]{3,}\s+\d{4}"))
                    if date_el: pub_date = self.parse_date(date_el)
                
                # FALLBACK: –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –±–µ—Ä–µ–º –¢–ï–ö–£–©–£–Æ
                if not pub_date:
                    pub_date = datetime.now()
                
                results.append({
                    "title": title,
                    "original_text": full_text or title,
                    "source_name": name,
                    "source_url": full_link,
                    "published_at": pub_date,
                    "image_url": image
                })
            logger.info(f"Direct {name}: {len(results)} items")
        except Exception as e:
            logger.error(f"Direct {name} Error: {e}")
        return results

    def fetch_details(self, url: str):
        try:
            resp = self.session.get(url, timeout=10, verify=False)
            soup = BeautifulSoup(resp.content, "html.parser")
            
            # –í–ê–ñ–ù–û: –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ get_text(), —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å DIV —Å –¥–∞—Ç–æ–π
            text = soup.get_text(separator="\n").strip()
            
            img = None
            meta_img = soup.find("meta", property="og:image")
            if meta_img: img = meta_img.get("content")

            pub_date = None
            # 1. –ú–µ—Ç–∞-—Ç–µ–≥–∏
            for prop in ["article:published_time", "published_time", "date"]:
                meta = soup.find("meta", property=prop) or soup.find("meta", attrs={"name": prop})
                if meta:
                    pub_date = self.parse_date(meta.get("content"))
                    if pub_date: break
            
            # 2. –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ (—Ç–µ–ø–µ—Ä—å –≤ –ø–æ–ª–Ω–æ–º —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
            if not pub_date:
                pub_date = self.find_date_in_text(text)

            return text, img, pub_date
        except:
            return None, None, None

scraper = Scraper()
