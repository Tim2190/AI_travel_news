import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import logging
import urllib3
import re
from typing import List, Dict, Optional, Tuple

# Playwright ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è gov.kz (–ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤)
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

# –û—Ç–∫–ª—é—á–∞–µ–º –Ω–∞–¥–æ–µ–¥–ª–∏–≤—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

# –ò–°–¢–û–ß–ù–ò–ö–ò: –û–§–ò–¶–ò–ê–õ–¨–ù–´–ï –°–ê–ô–¢–´ –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–• –û–†–ì–ê–ù–û–í (–†–£–°–°–ö–ò–ï –í–ï–†–°–ò–ò)
DIRECT_SCRAPE_SOURCES: List[Dict] = [
    # --- –í–´–°–®–ï–ï –†–£–ö–û–í–û–î–°–¢–í–û (–æ–±—ã—á–Ω—ã–π BS4 –ø–∞—Ä—Å–∏–Ω–≥, –Ω–µ SPA) ---
    {
        "name": "Akorda (–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç)",
        "url": "https://www.akorda.kz/ru/events",
        "article_selector": ".event-item, .news-list__item",
        "title_selector": "h3 a, .title a, a",
        "link_selector": "h3 a, .title a, a",
        "base_url": "https://www.akorda.kz",
        "gov_kz": False,
    },
    {
        "name": "PrimeMinister (–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ)",
        "url": "https://primeminister.kz/ru/news",
        "article_selector": ".news_item, .card, .post-item",
        "title_selector": ".news_title a, .card-title a, a",
        "link_selector": "a",
        "base_url": "https://primeminister.kz",
        "gov_kz": False,
    },

    # --- –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–ê (GOV.KZ - SPA, –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥) ---
    {
        "name": "–ú–∏–Ω–ù–∞—Ü–≠–∫–æ–Ω–æ–º–∏–∫–∏",
        "url": "https://www.gov.kz/memleket/entities/economy/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "economy",
    },
    {
        "name": "–ú–∏–Ω–§–∏–Ω",
        "url": "https://www.gov.kz/memleket/entities/minfin/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "minfin",
    },
    {
        "name": "–ú–ò–î –†–ö",
        "url": "https://www.gov.kz/memleket/entities/mfa/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mfa",
    },
    {
        "name": "–ú–í–î –†–ö",
        "url": "https://www.gov.kz/memleket/entities/qriim/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "qriim",
    },
    {
        "name": "–ú–∏–Ω–¢—Ä—É–¥–∞",
        "url": "https://www.gov.kz/memleket/entities/enbek/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "enbek",
    },
    {
        "name": "–ú–∏–Ω–ó–¥—Ä–∞–≤",
        "url": "https://www.gov.kz/memleket/entities/dsm/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "dsm",
    },
    {
        "name": "–ú–∏–Ω–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è",
        "url": "https://www.gov.kz/memleket/entities/edu/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "edu",
    },
    {
        "name": "–ú–∏–Ω–ù–∞—É–∫–∏",
        "url": "https://www.gov.kz/memleket/entities/sci/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "sci",
    },
    {
        "name": "–ú–∏–Ω–ü—Ä–æ–º–°—Ç—Ä–æ–π",
        "url": "https://www.gov.kz/memleket/entities/mps/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mps",
    },
    {
        "name": "–ú–∏–Ω–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç",
        "url": "https://www.gov.kz/memleket/entities/transport/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "transport",
    },
    {
        "name": "–ú–∏–Ω–¶–∏—Ñ—Ä—ã",
        "url": "https://www.gov.kz/memleket/entities/mdai/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mdai",
    },
    {
        "name": "–ú–∏–Ω–ö—É–ª—å—Ç—É—Ä—ã",
        "url": "https://www.gov.kz/memleket/entities/mam/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mam",
    },
    {
        "name": "–ú–∏–Ω–¢—É—Ä–∏–∑–º",
        "url": "https://www.gov.kz/memleket/entities/tsm/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "tsm",
    },
    {
        "name": "–ú–∏–Ω–≠–∫–æ–ª–æ–≥–∏–∏",
        "url": "https://www.gov.kz/memleket/entities/ecogeo/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "ecogeo",
    },
    {
        "name": "–ú–∏–Ω–°–µ–ª—å–•–æ–∑",
        "url": "https://www.gov.kz/memleket/entities/moa/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "moa",
    },
    {
        "name": "–ú–∏–Ω–≠–Ω–µ—Ä–≥–æ",
        "url": "https://www.gov.kz/memleket/entities/energo/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "energo",
    },
    {
        "name": "–ú–∏–Ω–Æ—Å—Ç",
        "url": "https://www.gov.kz/memleket/entities/adilet/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "adilet",
    },
    {
        "name": "–ú–ß–° –†–ö",
        "url": "https://www.gov.kz/memleket/entities/emer/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "emer",
    },
    {
        "name": "–ú–∏–Ω–¢–æ—Ä–≥–æ–≤–ª–∏",
        "url": "https://www.gov.kz/memleket/entities/mti/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mti",
    },

    # --- –ê–ö–ò–ú–ê–¢–´ –ú–ï–ì–ê–ü–û–õ–ò–°–û–í ---
    {
        "name": "–ê–∫–∏–º–∞—Ç –ê–ª–º–∞—Ç—ã",
        "url": "https://www.gov.kz/memleket/entities/almaty/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "almaty",
    },
    {
        "name": "–ê–∫–∏–º–∞—Ç –ê—Å—Ç–∞–Ω—ã",
        "url": "https://www.gov.kz/memleket/entities/astana/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "astana",
    },
]

# –ö—ç—à —Ç–æ–∫–µ–Ω–æ–≤ ‚Äî –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω —Ä–∞–∑ —á–µ—Ä–µ–∑ Playwright, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –≤—Å–µ—Ö gov.kz –∑–∞–ø—Ä–æ—Å–æ–≤
_gov_kz_tokens: Optional[Dict] = None


async def _fetch_gov_kz_tokens() -> Optional[Dict]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç Playwright –û–î–ò–ù –†–ê–ó, –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç hash+token,
    –∫–æ—Ç–æ—Ä—ã–µ –±—Ä–∞—É–∑–µ—Ä –ø–µ—Ä–µ–¥–∞—ë—Ç –≤ API gov.kz.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è requests.
    """
    if not PLAYWRIGHT_AVAILABLE:
        logger.error("Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤—å –≤ requirements.txt: playwright")
        return None

    tokens = {}
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=["--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage"]
            )
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="ru-RU",
            )
            page = await context.new_page()

            def handle_request(request):
                if "api/v1/public/content-manager/news" in request.url:
                    h = request.headers
                    if h.get("hash") and h.get("token"):
                        tokens["hash"] = h["hash"]
                        tokens["token"] = h["token"]
                        tokens["referer"] = h.get("referer", "https://www.gov.kz/")
                        tokens["user-agent"] = h.get("user-agent", "")
                        logger.info("‚úÖ gov.kz —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–µ–Ω—ã —á–µ—Ä–µ–∑ Playwright")

            page.on("request", handle_request)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º economy –∫–∞–∫ ¬´–¥–æ–Ω–æ—Ä–∞¬ª —Ç–æ–∫–µ–Ω–æ–≤ ‚Äî –æ–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
            await page.goto(
                "https://www.gov.kz/memleket/entities/economy/press/news?lang=ru",
                wait_until="domcontentloaded",
                timeout=60000,
            )
            await page.wait_for_selector(
                "a[href*='/press/news/details/']",
                timeout=30000,
            )
            await browser.close()

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ gov.kz: {e}")
        return None

    return tokens if tokens else None


class NewsScraper:
    def __init__(self, direct_sources: List[Dict] = None):
        self.direct_sources = direct_sources or DIRECT_SCRAPE_SOURCES

    # ========== –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´ 1: async/await –≤–º–µ—Å—Ç–æ loop.run_until_complete ==========
    async def scrape_async(self) -> List[Dict]:
        """
        Async-–≤–µ—Ä—Å–∏—è scrape() –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å FastAPI.
        –í—ã–∑—ã–≤–∞–π –µ—ë –∏–∑ FastAPI —Ç–∞–∫: await scraper.scrape_async()
        """
        all_news = []

        # –†–∞–∑–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ gov.kz –∏ –æ–±—ã—á–Ω—ã–µ
        gov_sources = [s for s in self.direct_sources if s.get("gov_kz")]
        regular_sources = [s for s in self.direct_sources if not s.get("gov_kz")]

        # –û–±—ã—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äî —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π BS4 (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å)
        import asyncio
        loop = asyncio.get_event_loop()
        regular_news = await loop.run_in_executor(
            None,
            self._scrape_all_regular_sources,
            regular_sources
        )
        all_news.extend(regular_news)

        # gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äî async –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥
        if gov_sources:
            gov_news = await self._scrape_all_gov_kz(gov_sources)
            all_news.extend(gov_news)

        logger.info(f"Total news gathered: {len(all_news)}")
        return all_news

    def _scrape_all_regular_sources(self, sources: List[Dict]) -> List[Dict]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–µ gov.kz)"""
        all_news = []
        for source in sources:
            all_news.extend(self._scrape_direct_source(source))
        return all_news

    async def _scrape_all_gov_kz(self, sources: List[Dict]) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –û–î–ò–ù –†–ê–ó —á–µ—Ä–µ–∑ Playwright,
        –∑–∞—Ç–µ–º –æ–±—Ö–æ–¥–∏—Ç –≤—Å–µ gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ –ª—ë–≥–∫–∏–π requests.
        """
        global _gov_kz_tokens

        if _gov_kz_tokens is None:
            logger.info("üîë –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã gov.kz —á–µ—Ä–µ–∑ Playwright...")
            _gov_kz_tokens = await _fetch_gov_kz_tokens()

        if not _gov_kz_tokens:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω—ã gov.kz ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å–µ gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
            return []

        all_news = []
        for source in sources:
            all_news.extend(self._scrape_gov_kz_source(source, _gov_kz_tokens))

        return all_news

    def _scrape_gov_kz_source(self, config: Dict, tokens: Dict) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫ —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π API –∑–∞–ø—Ä–æ—Å —Å —Ç–æ–∫–µ–Ω–∞–º–∏.
        –ë—Ä–∞—É–∑–µ—Ä –∑–¥–µ—Å—å –Ω–µ –Ω—É–∂–µ–Ω ‚Äî —Ç–æ–ª—å–∫–æ –ª—ë–≥–∫–∏–π requests.
        """
        name = config.get("name", "Unknown")
        project = config.get("project")
        base_url = config.get("base_url", "https://www.gov.kz")

        if not project:
            logger.warning(f"'{name}' –ø—Ä–æ–ø—É—â–µ–Ω: –Ω–µ —É–∫–∞–∑–∞–Ω 'project'")
            return []

        api_url = (
            f"https://www.gov.kz/api/v1/public/content-manager/news"
            f"?sort-by=created_date:DESC&projects=eq:{project}&page=1&size=20"
        )

        headers = {
            "accept": "application/json",
            "accept-language": "ru",
            "user-agent": tokens.get("user-agent", "Mozilla/5.0"),
            "referer": f"{base_url}/memleket/entities/{project}/press/news?lang=ru",
            "hash": tokens["hash"],
            "token": tokens["token"],
        }

        news = []
        try:
            logger.info(f"API –∑–∞–ø—Ä–æ—Å: {name}...")
            resp = requests.get(api_url, headers=headers, timeout=15, verify=False)
            resp.raise_for_status()
            data = resp.json()

            # API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç {"content": [...], "totalElements": N, ...}
            items = data.get("content", [])

            for item in items:
                title = item.get("name", "").strip()
                slug = item.get("id") or item.get("slug", "")
                if not title or not slug:
                    continue

                link = f"{base_url}/memleket/entities/{project}/press/news/details/{slug}?lang=ru"

                # –î–∞—Ç–∞ –∏–∑ API ‚Äî —É–∂–µ –µ—Å—Ç—å, –Ω–µ –Ω—É–∂–Ω–æ –ø–∞—Ä—Å–∏—Ç—å HTML
                published_at = None
                raw_date = item.get("createdDate") or item.get("created_date") or item.get("publishedDate")
                if raw_date:
                    published_at = self._parse_date(str(raw_date))

                # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∫–∞—Ä—Ç–∏–Ω–∫—É –±–µ—Ä—ë–º —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏
                full_text, image_url, page_date = self._fetch_full_text_and_image(link)

                # –í–ê–ñ–ù–û: –µ—Å–ª–∏ –¥–∞—Ç–∞ –∏–∑ API –ø—É—Å—Ç–∞—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                final_date = published_at or page_date

                news.append({
                    "title": title,
                    "original_text": full_text or title,
                    "source_name": name,
                    "source_url": link,
                    "image_url": image_url,
                    "published_at": final_date,
                })

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ API {name}: {e}")

        return news

    def _scrape_direct_source(self, config: Dict) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ —Å–ø–∏—Å–∫–æ–º —Å—Ç–∞—Ç–µ–π –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –Ω–µ-SPA)."""
        name = config.get("name", "Unknown")
        url = config.get("url")
        article_sel = config.get("article_selector")
        title_sel = config.get("title_selector")
        link_sel = config.get("link_selector", "a")
        base_url = config.get("base_url", "").rstrip("/")

        if not url or not article_sel or not title_sel:
            logger.warning(f"Direct source '{name}' skipped: missing config")
            return []

        news = []
        try:
            logger.info(f"Direct scraping {name}...")
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"}
            resp = requests.get(url, headers=headers, timeout=15, verify=False)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.content, "html.parser")
            articles = soup.select(article_sel)[:20]

            for art in articles:
                title_el = art.select_one(title_sel)
                link_el = art.select_one(link_sel) if link_sel else title_el

                if not title_el or not link_el:
                    continue

                title = title_el.get_text(strip=True)
                href = link_el.get("href", "")

                if not href:
                    continue

                link = (base_url + href) if href.startswith("/") else href
                if not link.startswith("http"):
                    link = base_url + "/" + link

                full_text, image_url, published_at = self._fetch_full_text_and_image(link)

                news.append({
                    "title": title,
                    "original_text": full_text or title,
                    "source_name": name,
                    "source_url": link,
                    "image_url": image_url,
                    "published_at": published_at,
                })
        except Exception as e:
            logger.error(f"Error scraping {name}: {e}")
        return news

    # ========== –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´ 2: –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç ==========
    def _extract_publish_date(self, soup: BeautifulSoup) -> Optional[datetime]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑:
        1. –ú–µ—Ç–∞-—Ç–µ–≥–æ–≤ (og:published_time –∏ —Ç.–¥.)
        2. <time datetime="">
        3. –í–∏–¥–∏–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è)
        """
        # 1. –ü—Ä–æ–±—É–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏
        for prop in ("article:published_time", "published_time", "date", "og:updated_time"):
            meta = soup.find("meta", property=prop) or soup.find("meta", attrs={"name": prop})
            if meta and meta.get("content"):
                parsed = self._parse_date(meta["content"])
                if parsed:
                    return parsed

        # 2. –ü—Ä–æ–±—É–µ–º <time datetime="">
        time_el = soup.find("time", attrs={"datetime": True})
        if time_el and time_el.get("datetime"):
            parsed = self._parse_date(time_el["datetime"])
            if parsed:
                return parsed

        # 3. –ò—â–µ–º –¥–∞—Ç—É –≤ –≤–∏–¥–∏–º–æ–º —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ regex
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã: "18 —Ñ–µ–≤—Ä–∞–ª—è 2025", "18.02.2025", "2025-02-18"
        text = soup.get_text()
        date_from_text = self._extract_date_from_text(text)
        if date_from_text:
            return date_from_text

        return None

    def _extract_date_from_text(self, text: str) -> Optional[datetime]:
        """
        –ò—â–µ—Ç –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã:
        - "18 —Ñ–µ–≤—Ä–∞–ª—è 2025"
        - "18.02.2025"
        - "2025-02-18"
        """
        # –ú–µ—Å—è—Ü—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        months_ru = {
            "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4,
            "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8,
            "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12
        }

        # 1. –§–æ—Ä–º–∞—Ç "18 —Ñ–µ–≤—Ä–∞–ª—è 2025"
        pattern1 = r"(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})"
        match = re.search(pattern1, text, re.IGNORECASE)
        if match:
            day = int(match.group(1))
            month = months_ru[match.group(2).lower()]
            year = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        # 2. –§–æ—Ä–º–∞—Ç "18.02.2025" –∏–ª–∏ "18/02/2025"
        pattern2 = r"(\d{1,2})[./](\d{1,2})[./](\d{4})"
        match = re.search(pattern2, text)
        if match:
            day = int(match.group(1))
            month = int(match.group(2))
            year = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        # 3. –§–æ—Ä–º–∞—Ç ISO "2025-02-18"
        pattern3 = r"(\d{4})-(\d{1,2})-(\d{1,2})"
        match = re.search(pattern3, text)
        if match:
            year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            try:
                return datetime(year, month, day)
            except ValueError:
                pass

        return None

    def _parse_date(self, value: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç ISO –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫–∏."""
        if not value or not value.strip():
            return None
        value = value.strip()[:25]
        for fmt in ("%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
            try:
                if value.endswith("Z"):
                    value = value[:-1] + "+00:00"
                if "+" in value or value.count("-") >= 2:
                    d = datetime.fromisoformat(value.replace("Z", "+00:00"))
                else:
                    d = datetime.strptime(value[:10], "%Y-%m-%d")
                if d.tzinfo:
                    d = d.astimezone(timezone.utc).replace(tzinfo=None)
                return d
            except Exception:
                continue
        return None

    def _fetch_full_text_and_image(self, url: str) -> Tuple[Optional[str], Optional[str], Optional[datetime]]:
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            if response.status_code != 200:
                return None, None, None
            soup = BeautifulSoup(response.content, "html.parser")

            paragraphs = soup.find_all("p")
            text = "\n".join([p.get_text() for p in paragraphs if len(p.get_text()) > 50])

            image_url = None
            og = soup.find("meta", property="og:image")
            if og and og.get("content"):
                image_url = og.get("content")
            if not image_url:
                img = soup.find("img")
                if img and img.get("src"):
                    image_url = img.get("src")

            published_at = self._extract_publish_date(soup)
            return text, image_url, published_at
        except Exception:
            return None, None, None


scraper = NewsScraper()
