import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import logging
import urllib3
from typing import List, Dict, Optional, Tuple
import asyncio

# Playwright â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ gov.kz (Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²)
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

# ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð½Ð°Ð´Ð¾ÐµÐ´Ð»Ð¸Ð²Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ñ Ð¾ SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

# Ð˜Ð¡Ð¢ÐžÐ§ÐÐ˜ÐšÐ˜: ÐžÐ¤Ð˜Ð¦Ð˜ÐÐ›Ð¬ÐÐ«Ð• Ð¡ÐÐ™Ð¢Ð« Ð“ÐžÐ¡Ð£Ð”ÐÐ Ð¡Ð¢Ð’Ð•ÐÐÐ«Ð¥ ÐžÐ Ð“ÐÐÐžÐ’ (Ð Ð£Ð¡Ð¡ÐšÐ˜Ð• Ð’Ð•Ð Ð¡Ð˜Ð˜)
DIRECT_SCRAPE_SOURCES: List[Dict] = [
    # --- Ð’Ð«Ð¡Ð¨Ð•Ð• Ð Ð£ÐšÐžÐ’ÐžÐ”Ð¡Ð¢Ð’Ðž (Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ BS4 Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³, Ð½Ðµ SPA) ---
    {
        "name": "Akorda (ÐŸÑ€ÐµÐ·Ð¸Ð´ÐµÐ½Ñ‚)",
        "url": "https://www.akorda.kz/ru/events",
        "article_selector": ".event-item, .news-list__item",
        "title_selector": "h3 a, .title a, a",
        "link_selector": "h3 a, .title a, a",
        "base_url": "https://www.akorda.kz",
        "gov_kz": False,
    },
    {
        "name": "PrimeMinister (ÐŸÑ€Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾)",
        "url": "https://primeminister.kz/ru/news",
        "article_selector": ".news_item, .card, .post-item",
        "title_selector": ".news_title a, .card-title a, a",
        "link_selector": "a",
        "base_url": "https://primeminister.kz",
        "gov_kz": False,
    },

    # --- ÐœÐ˜ÐÐ˜Ð¡Ð¢Ð•Ð Ð¡Ð¢Ð’Ð (GOV.KZ - SPA, Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´) ---
    {
        "name": "ÐœÐ¸Ð½ÐÐ°Ñ†Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ¸",
        "url": "https://www.gov.kz/memleket/entities/economy/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "economy",
    },
    {
        "name": "ÐœÐ¸Ð½Ð¤Ð¸Ð½",
        "url": "https://www.gov.kz/memleket/entities/minfin/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "minfin",
    },
    {
        "name": "ÐœÐ˜Ð” Ð Ðš",
        "url": "https://www.gov.kz/memleket/entities/mfa/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mfa",
    },
    {
        "name": "ÐœÐ’Ð” Ð Ðš",
        "url": "https://www.gov.kz/memleket/entities/qriim/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "qriim",
    },
    {
        "name": "ÐœÐ¸Ð½Ð¢Ñ€ÑƒÐ´Ð°",
        "url": "https://www.gov.kz/memleket/entities/enbek/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "enbek",
    },
    {
        "name": "ÐœÐ¸Ð½Ð—Ð´Ñ€Ð°Ð²",
        "url": "https://www.gov.kz/memleket/entities/dsm/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "dsm",
    },
    {
        "name": "ÐœÐ¸Ð½ÐŸÑ€Ð¾ÑÐ²ÐµÑ‰ÐµÐ½Ð¸Ñ",
        "url": "https://www.gov.kz/memleket/entities/edu/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "edu",
    },
    {
        "name": "ÐœÐ¸Ð½ÐÐ°ÑƒÐºÐ¸",
        "url": "https://www.gov.kz/memleket/entities/sci/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "sci",
    },
    {
        "name": "ÐœÐ¸Ð½ÐŸÑ€Ð¾Ð¼Ð¡Ñ‚Ñ€Ð¾Ð¹",
        "url": "https://www.gov.kz/memleket/entities/mps/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mps",
    },
    {
        "name": "ÐœÐ¸Ð½Ð¢Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚",
        "url": "https://www.gov.kz/memleket/entities/transport/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "transport",
    },
    {
        "name": "ÐœÐ¸Ð½Ð¦Ð¸Ñ„Ñ€Ñ‹",
        "url": "https://www.gov.kz/memleket/entities/mdai/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mdai",
    },
    {
        "name": "ÐœÐ¸Ð½ÐšÑƒÐ»ÑŒÑ‚ÑƒÑ€Ñ‹",
        "url": "https://www.gov.kz/memleket/entities/mam/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mam",
    },
    {
        "name": "ÐœÐ¸Ð½Ð¢ÑƒÑ€Ð¸Ð·Ð¼",
        "url": "https://www.gov.kz/memleket/entities/tsm/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "tsm",
    },
    {
        "name": "ÐœÐ¸Ð½Ð­ÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸",
        "url": "https://www.gov.kz/memleket/entities/ecogeo/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "ecogeo",
    },
    {
        "name": "ÐœÐ¸Ð½Ð¡ÐµÐ»ÑŒÐ¥Ð¾Ð·",
        "url": "https://www.gov.kz/memleket/entities/moa/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "moa",
    },
    {
        "name": "ÐœÐ¸Ð½Ð­Ð½ÐµÑ€Ð³Ð¾",
        "url": "https://www.gov.kz/memleket/entities/energo/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "energo",
    },
    {
        "name": "ÐœÐ¸Ð½Ð®ÑÑ‚",
        "url": "https://www.gov.kz/memleket/entities/adilet/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "adilet",
    },
    {
        "name": "ÐœÐ§Ð¡ Ð Ðš",
        "url": "https://www.gov.kz/memleket/entities/emer/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "emer",
    },
    {
        "name": "ÐœÐ¸Ð½Ð¢Ð¾Ñ€Ð³Ð¾Ð²Ð»Ð¸",
        "url": "https://www.gov.kz/memleket/entities/mti/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "mti",
    },

    # --- ÐÐšÐ˜ÐœÐÐ¢Ð« ÐœÐ•Ð“ÐÐŸÐžÐ›Ð˜Ð¡ÐžÐ’ ---
    {
        "name": "ÐÐºÐ¸Ð¼Ð°Ñ‚ ÐÐ»Ð¼Ð°Ñ‚Ñ‹",
        "url": "https://www.gov.kz/memleket/entities/almaty/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "almaty",
    },
    {
        "name": "ÐÐºÐ¸Ð¼Ð°Ñ‚ ÐÑÑ‚Ð°Ð½Ñ‹",
        "url": "https://www.gov.kz/memleket/entities/astana/press/news?lang=ru",
        "base_url": "https://www.gov.kz",
        "gov_kz": True,
        "project": "astana",
    },
]

# ÐšÑÑˆ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² â€” Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð· Ñ‡ÐµÑ€ÐµÐ· Playwright, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð´Ð»Ñ Ð²ÑÐµÑ… gov.kz Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
_gov_kz_tokens: Optional[Dict] = None


async def _fetch_gov_kz_tokens() -> Optional[Dict]:
    """
    Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Playwright ÐžÐ”Ð˜Ð Ð ÐÐ—, Ð¿ÐµÑ€ÐµÑ…Ð²Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ hash+token,
    ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‘Ñ‚ Ð² API gov.kz.
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼Ð¸ Ð´Ð»Ñ requests.
    """
    if not PLAYWRIGHT_AVAILABLE:
        logger.error("Playwright Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½. Ð”Ð¾Ð±Ð°Ð²ÑŒ Ð² requirements.txt: playwright")
        return None

    tokens = {}
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=["--no-sandbox", "--disable-setuid-sandbox"]
            )
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="ru-RU",
            )
            page = await context.new_page()

            def handle_request(request):
                if "api/v1/public/content-manager/news" in request.url:
                    h = request.headers
                    if h.get("hash") and h.get("token"):
                        tokens["hash"] = h["hash"]
                        tokens["token"] = h["token"]
                        tokens["referer"] = h.get("referer", "https://www.gov.kz/")
                        tokens["user-agent"] = h.get("user-agent", "")
                        logger.info("âœ… gov.kz Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ñ‹ Ñ‡ÐµÑ€ÐµÐ· Playwright")

            page.on("request", handle_request)

            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ economy ÐºÐ°Ðº Â«Ð´Ð¾Ð½Ð¾Ñ€Ð°Â» Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² â€” Ð¾Ð½Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ð²
            await page.goto(
                "https://www.gov.kz/memleket/entities/economy/press/news?lang=ru",
                wait_until="domcontentloaded",
                timeout=60000,
            )
            await page.wait_for_selector(
                "a[href*='/press/news/details/']",
                timeout=30000,
            )
            await browser.close()

    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² gov.kz: {e}")
        return None

    return tokens if tokens else None


class NewsScraper:
    def __init__(self, direct_sources: List[Dict] = None):
        self.direct_sources = direct_sources or DIRECT_SCRAPE_SOURCES

    def scrape(self) -> List[Dict]:
        all_news = []

        # Ð Ð°Ð·Ð´ÐµÐ»ÑÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð½Ð° gov.kz Ð¸ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ðµ
        gov_sources = [s for s in self.direct_sources if s.get("gov_kz")]
        regular_sources = [s for s in self.direct_sources if not s.get("gov_kz")]

        # ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ â€” ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ BS4
        for source in regular_sources:
            all_news.extend(self._scrape_direct_source(source))

        # gov.kz Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ â€” Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ñ‡ÐµÑ€ÐµÐ· API
        if gov_sources:
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            all_news.extend(loop.run_until_complete(self._scrape_all_gov_kz(gov_sources)))

        logger.info(f"Total news gathered: {len(all_news)}")
        return all_news

    async def _scrape_all_gov_kz(self, sources: List[Dict]) -> List[Dict]:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ ÐžÐ”Ð˜Ð Ð ÐÐ— Ñ‡ÐµÑ€ÐµÐ· Playwright,
        Ð·Ð°Ñ‚ÐµÐ¼ Ð¾Ð±Ñ…Ð¾Ð´Ð¸Ñ‚ Ð²ÑÐµ gov.kz Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ñ‡ÐµÑ€ÐµÐ· Ð»Ñ‘Ð³ÐºÐ¸Ð¹ requests.
        """
        global _gov_kz_tokens

        if _gov_kz_tokens is None:
            logger.info("ðŸ”‘ ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ gov.kz Ñ‡ÐµÑ€ÐµÐ· Playwright...")
            _gov_kz_tokens = await _fetch_gov_kz_tokens()

        if not _gov_kz_tokens:
            logger.error("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ gov.kz â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð²ÑÐµ gov.kz Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸")
            return []

        all_news = []
        for source in sources:
            all_news.extend(self._scrape_gov_kz_source(source, _gov_kz_tokens))

        return all_news

    def _scrape_gov_kz_source(self, config: Dict, tokens: Dict) -> List[Dict]:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ñ‚ Ð¾Ð´Ð¸Ð½ gov.kz Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ñ‡ÐµÑ€ÐµÐ· Ð¿Ñ€ÑÐ¼Ð¾Ð¹ API Ð·Ð°Ð¿Ñ€Ð¾Ñ Ñ Ñ‚Ð¾ÐºÐµÐ½Ð°Ð¼Ð¸.
        Ð‘Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð·Ð´ÐµÑÑŒ Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½ â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð»Ñ‘Ð³ÐºÐ¸Ð¹ requests.
        """
        name = config.get("name", "Unknown")
        project = config.get("project")
        base_url = config.get("base_url", "https://www.gov.kz")

        if not project:
            logger.warning(f"'{name}' Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½: Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½ 'project'")
            return []

        api_url = (
            f"https://www.gov.kz/api/v1/public/content-manager/news"
            f"?sort-by=created_date:DESC&projects=eq:{project}&page=1&size=20"
        )

        headers = {
            "accept": "application/json",
            "accept-language": "ru",
            "user-agent": tokens.get("user-agent", "Mozilla/5.0"),
            "referer": f"{base_url}/memleket/entities/{project}/press/news?lang=ru",
            "hash": tokens["hash"],
            "token": tokens["token"],
        }

        news = []
        try:
            logger.info(f"API Ð·Ð°Ð¿Ñ€Ð¾Ñ: {name}...")
            resp = requests.get(api_url, headers=headers, timeout=15, verify=False)
            resp.raise_for_status()
            data = resp.json()

            # API Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ {"content": [...], "totalElements": N, ...}
            items = data.get("content", [])

            for item in items:
                title = item.get("name", "").strip()
                slug = item.get("id") or item.get("slug", "")
                if not title or not slug:
                    continue

                link = f"{base_url}/memleket/entities/{project}/press/news/details/{slug}?lang=ru"

                # Ð”Ð°Ñ‚Ð° Ð¸Ð· API â€” ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ, Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ HTML
                published_at = None
                raw_date = item.get("createdDate") or item.get("created_date") or item.get("publishedDate")
                if raw_date:
                    published_at = self._parse_date(str(raw_date))

                # ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¸ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ Ð±ÐµÑ€Ñ‘Ð¼ ÑÐ¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ ÑÑ‚Ð°Ñ‚ÑŒÐ¸
                full_text, image_url, page_date = self._fetch_full_text_and_image(link)

                news.append({
                    "title": title,
                    "original_text": full_text or title,
                    "source_name": name,
                    "source_url": link,
                    "image_url": image_url,
                    "published_at": published_at or page_date,
                })

        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° API {name}: {e}")

        return news

    def _scrape_direct_source(self, config: Dict) -> List[Dict]:
        """ÐŸÐ°Ñ€ÑÐ¸Ñ‚ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ ÑÐ¾ ÑÐ¿Ð¸ÑÐºÐ¾Ð¼ ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð¿Ð¾ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð°Ð¼ (ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð½Ðµ-SPA)."""
        name = config.get("name", "Unknown")
        url = config.get("url")
        article_sel = config.get("article_selector")
        title_sel = config.get("title_selector")
        link_sel = config.get("link_selector", "a")
        base_url = config.get("base_url", "").rstrip("/")

        if not url or not article_sel or not title_sel:
            logger.warning(f"Direct source '{name}' skipped: missing config")
            return []

        news = []
        try:
            logger.info(f"Direct scraping {name}...")
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"}
            resp = requests.get(url, headers=headers, timeout=15, verify=False)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.content, "html.parser")
            articles = soup.select(article_sel)[:20]

            for art in articles:
                title_el = art.select_one(title_sel)
                link_el = art.select_one(link_sel) if link_sel else title_el

                if not title_el or not link_el:
                    continue

                title = title_el.get_text(strip=True)
                href = link_el.get("href", "")

                if not href:
                    continue

                link = (base_url + href) if href.startswith("/") else href
                if not link.startswith("http"):
                    link = base_url + "/" + link

                full_text, image_url, published_at = self._fetch_full_text_and_image(link)

                news.append({
                    "title": title,
                    "original_text": full_text or title,
                    "source_name": name,
                    "source_url": link,
                    "image_url": image_url,
                    "published_at": published_at,
                })
        except Exception as e:
            logger.error(f"Error scraping {name}: {e}")
        return news

    def _extract_publish_date(self, soup: BeautifulSoup) -> Optional[datetime]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð´Ð°Ñ‚Ñƒ Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸."""
        for prop in ("article:published_time", "published_time", "date"):
            meta = soup.find("meta", property=prop) or soup.find("meta", attrs={"name": prop})
            if meta and meta.get("content"):
                return self._parse_date(meta["content"])
        time_el = soup.find("time", attrs={"datetime": True})
        if time_el and time_el.get("datetime"):
            return self._parse_date(time_el["datetime"])
        return None

    def _parse_date(self, value: str) -> Optional[datetime]:
        if not value or not value.strip():
            return None
        value = value.strip()[:25]
        for fmt in ("%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
            try:
                if value.endswith("Z"):
                    value = value[:-1] + "+00:00"
                if "+" in value or value.count("-") >= 2:
                    d = datetime.fromisoformat(value.replace("Z", "+00:00"))
                else:
                    d = datetime.strptime(value[:10], "%Y-%m-%d")
                if d.tzinfo:
                    d = d.astimezone(timezone.utc).replace(tzinfo=None)
                return d
            except Exception:
                continue
        return None

    def _fetch_full_text_and_image(self, url: str) -> Tuple[Optional[str], Optional[str], Optional[datetime]]:
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            if response.status_code != 200:
                return None, None, None
            soup = BeautifulSoup(response.content, "html.parser")

            paragraphs = soup.find_all("p")
            text = "\n".join([p.get_text() for p in paragraphs if len(p.get_text()) > 50])

            image_url = None
            og = soup.find("meta", property="og:image")
            if og and og.get("content"):
                image_url = og.get("content")
            if not image_url:
                img = soup.find("img")
                if img and img.get("src"):
                    image_url = img.get("src")

            published_at = self._extract_publish_date(soup)
            return text, image_url, published_at
        except Exception:
            return None, None, None


scraper = NewsScraper()
