import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import logging
import urllib3
from typing import List, Dict, Optional, Tuple
import asyncio
from concurrent.futures import ThreadPoolExecutor

# Playwright ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è gov.kz (–ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤)
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

# –û—Ç–∫–ª—é—á–∞–µ–º –Ω–∞–¥–æ–µ–¥–ª–∏–≤—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

# –ò–°–¢–û–ß–ù–ò–ö–ò: –û–§–ò–¶–ò–ê–õ–¨–ù–´–ï –°–ê–ô–¢–´ –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–• –û–†–ì–ê–ù–û–í (–†–£–°–°–ö–ò–ï –í–ï–†–°–ò–ò)
DIRECT_SCRAPE_SOURCES: List[Dict] = [
    # --- –í–´–°–®–ï–ï –†–£–ö–û–í–û–î–°–¢–í–û ---
    {
        "name": "Akorda (–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç)",
        "url": "https://www.akorda.kz/ru/events",
        "article_selector": ".event-item, .news-list__item",
        "title_selector": "h3 a, .title a, a",
        "link_selector": "h3 a, .title a, a",
        "base_url": "https://www.akorda.kz",
        "gov_kz": False,
    },
    {
        "name": "PrimeMinister (–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ)",
        "url": "https://primeminister.kz/ru/news",
        "article_selector": ".news_item, .card, .post-item",
        "title_selector": ".news_title a, .card-title a, a",
        "link_selector": "a",
        "base_url": "https://primeminister.kz",
        "gov_kz": False,
    },
    # --- –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–ê (GOV.KZ - SPA) ---
    {"name": "–ú–∏–Ω–ù–∞—Ü–≠–∫–æ–Ω–æ–º–∏–∫–∏", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "economy"},
    {"name": "–ú–∏–Ω–§–∏–Ω", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "minfin"},
    {"name": "–ú–ò–î –†–ö", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "mfa"},
    {"name": "–ú–í–î –†–ö", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "qriim"},
    {"name": "–ú–∏–Ω–¢—Ä—É–¥–∞", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "enbek"},
    {"name": "–ú–∏–Ω–ó–¥—Ä–∞–≤", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "dsm"},
    {"name": "–ú–∏–Ω–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "edu"},
    {"name": "–ú–∏–Ω–ù–∞—É–∫–∏", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "sci"},
    {"name": "–ú–∏–Ω–ü—Ä–æ–º–°—Ç—Ä–æ–π", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "mps"},
    {"name": "–ú–∏–Ω–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "transport"},
    {"name": "–ú–∏–Ω–¶–∏—Ñ—Ä—ã", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "mdai"},
    {"name": "–ú–∏–Ω–ö—É–ª—å—Ç—É—Ä—ã", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "mam"},
    {"name": "–ú–∏–Ω–¢—É—Ä–∏–∑–º", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "tsm"},
    {"name": "–ú–∏–Ω–≠–∫–æ–ª–æ–≥–∏–∏", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "ecogeo"},
    {"name": "–ú–∏–Ω–°–µ–ª—å–•–æ–∑", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "moa"},
    {"name": "–ú–∏–Ω–≠–Ω–µ—Ä–≥–æ", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "energo"},
    {"name": "–ú–∏–Ω–Æ—Å—Ç", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "adilet"},
    {"name": "–ú–ß–° –†–ö", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "emer"},
    {"name": "–ú–∏–Ω–¢–æ—Ä–≥–æ–≤–ª–∏", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "mti"},
    # --- –ê–ö–ò–ú–ê–¢–´ ---
    {"name": "–ê–∫–∏–º–∞—Ç –ê–ª–º–∞—Ç—ã", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "almaty"},
    {"name": "–ê–∫–∏–º–∞—Ç –ê—Å—Ç–∞–Ω—ã", "url": "...", "base_url": "https://www.gov.kz", "gov_kz": True, "project": "astana"},
]

_gov_kz_tokens: Optional[Dict] = None

async def _fetch_gov_kz_tokens() -> Optional[Dict]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (–∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ)"""
    if not PLAYWRIGHT_AVAILABLE:
        return None
    tokens = {}
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-setuid-sandbox"])
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080}, locale="ru-RU"
            )
            page = await context.new_page()
            
            # –õ–æ–≤—É—à–∫–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤
            def handle_request(request):
                if "api/v1/public/content-manager/news" in request.url:
                    h = request.headers
                    if h.get("hash") and h.get("token"):
                        tokens["hash"] = h["hash"]
                        tokens["token"] = h["token"]
                        tokens["user-agent"] = h.get("user-agent", "")
            
            page.on("request", handle_request)
            
            # –ó–∞—Ö–æ–¥–∏–º –Ω–∞ —Å–∞–π—Ç
            try:
                await page.goto("https://www.gov.kz/memleket/entities/economy/press/news?lang=ru", timeout=45000, wait_until="domcontentloaded")
                # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ, —á—Ç–æ–±—ã —Å–∫—Ä–∏–ø—Ç—ã –æ—Ç—Ä–∞–±–æ—Ç–∞–ª–∏
                await page.wait_for_timeout(5000) 
            except Exception:
                pass # –ì–ª–∞–≤–Ω–æ–µ, —á—Ç–æ–±—ã —É—Å–ø–µ–ª–∏ –ø–µ—Ä–µ—Ö–≤–∞—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å
            
            await browser.close()
    except Exception as e:
        logger.error(f"Playwright error: {e}")
        return None
    return tokens if tokens else None

def run_async_in_thread(coro):
    """–ú–∞–≥–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∑–∞–ø—É—Å–∫–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–¥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª"""
    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(asyncio.run, coro)
        return future.result()

class NewsScraper:
    def __init__(self, direct_sources: List[Dict] = None):
        self.direct_sources = direct_sources or DIRECT_SCRAPE_SOURCES

    def scrape(self) -> List[Dict]:
        all_news = []
        
        # 1. –û–±—ã—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
        regular_sources = [s for s in self.direct_sources if not s.get("gov_kz")]
        for source in regular_sources:
            all_news.extend(self._scrape_direct_source(source))

        # 2. Gov.kz –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (Playwright –∑–∞–ø—É—Å–∫–∞–µ–º –≤ –ø–æ—Ç–æ–∫–µ)
        gov_sources = [s for s in self.direct_sources if s.get("gov_kz")]
        if gov_sources:
            try:
                # –í–ê–ñ–ù–û: –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —á–∞—Å—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ!
                # –≠—Ç–æ –æ–±—Ö–æ–¥–∏—Ç –æ—à–∏–±–∫—É "Event loop is already running"
                gov_news = run_async_in_thread(self._scrape_all_gov_kz(gov_sources))
                all_news.extend(gov_news)
            except Exception as e:
                logger.error(f"Critical error scraping gov.kz: {e}")

        logger.info(f"Total news gathered: {len(all_news)}")
        return all_news

    async def _scrape_all_gov_kz(self, sources: List[Dict]) -> List[Dict]:
        global _gov_kz_tokens
        if _gov_kz_tokens is None:
            logger.info("üîë –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã gov.kz (–≤ –ø–æ—Ç–æ–∫–µ)...")
            _gov_kz_tokens = await _fetch_gov_kz_tokens()

        if not _gov_kz_tokens:
            logger.error("–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ gov.kz")
            return []

        all_news = []
        for source in sources:
            all_news.extend(self._scrape_gov_kz_source(source, _gov_kz_tokens))
        return all_news

    def _scrape_gov_kz_source(self, config: Dict, tokens: Dict) -> List[Dict]:
        """API –∑–∞–ø—Ä–æ—Å (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π requests)"""
        name = config.get("name", "Unknown")
        project = config.get("project")
        if not project: return []

        api_url = f"https://www.gov.kz/api/v1/public/content-manager/news?sort-by=created_date:DESC&projects=eq:{project}&page=1&size=10"
        headers = {
            "accept": "application/json", "accept-language": "ru",
            "user-agent": tokens.get("user-agent", "Mozilla/5.0"),
            "referer": f"https://www.gov.kz/memleket/entities/{project}/press/news?lang=ru",
            "hash": tokens["hash"], "token": tokens["token"],
        }
        
        news = []
        try:
            logger.info(f"API –∑–∞–ø—Ä–æ—Å: {name}...")
            resp = requests.get(api_url, headers=headers, timeout=15, verify=False)
            if resp.status_code == 200:
                items = resp.json().get("content", [])
                for item in items:
                    title = item.get("name")
                    slug_id = item.get("id")
                    if title and slug_id:
                        link = f"https://www.gov.kz/memleket/entities/{project}/press/news/details/{slug_id}?lang=ru"
                        
                        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É API
                        raw_date = item.get("createdDate") or item.get("publishedDate")
                        pub_date = self._parse_date(str(raw_date)) if raw_date else datetime.now()

                        # –¢–µ–∫—Å—Ç –±–µ—Ä–µ–º –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏ (–ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
                        full_text, image, _ = self._fetch_full_text_and_image(link)
                        
                        news.append({
                            "title": title, "original_text": full_text or title,
                            "source_name": name, "source_url": link,
                            "image_url": image, "published_at": pub_date
                        })
        except Exception as e:
            logger.error(f"Error {name}: {e}")
        return news

    def _scrape_direct_source(self, config: Dict) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–±—ã—á–Ω—ã—Ö HTML —Å–∞–π—Ç–æ–≤"""
        name = config.get("name", "Unknown")
        url = config.get("url")
        if not url: return []
        
        news = []
        try:
            logger.info(f"Direct scraping {name}...")
            headers = {"User-Agent": "Mozilla/5.0"}
            resp = requests.get(url, headers=headers, timeout=15, verify=False)
            soup = BeautifulSoup(resp.content, "html.parser")
            
            articles = soup.select(config.get("article_selector"))[:10]
            for art in articles:
                title_el = art.select_one(config.get("title_selector"))
                link_el = art.select_one(config.get("link_selector", "a"))
                
                if title_el:
                    title = title_el.get_text(strip=True)
                    href = link_el.get("href") if link_el else title_el.get("href")
                    if href:
                        base = config.get("base_url", "")
                        link = base + href if href.startswith("/") else href
                        
                        full_text, image, pub_date = self._fetch_full_text_and_image(link)
                        news.append({
                            "title": title, "original_text": full_text or title,
                            "source_name": name, "source_url": link,
                            "image_url": image, "published_at": pub_date
                        })
        except Exception as e:
            logger.error(f"Error {name}: {e}")
        return news

    def _fetch_full_text_and_image(self, url):
        try:
            resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15, verify=False)
            if resp.status_code != 200: return None, None, None
            soup = BeautifulSoup(resp.content, "html.parser")
            paragraphs = soup.find_all("p")
            text = "\n".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text()) > 50])
            
            img = soup.find("meta", property="og:image")
            image_url = img.get("content") if img else None
            
            # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –¥–∞—Ç—É
            pub_date = self._extract_publish_date(soup)
            return text, image_url, pub_date
        except:
            return None, None, None

    def _extract_publish_date(self, soup):
        # ... (–¢–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
        return datetime.now() 

    def _parse_date(self, value):
        # ... (–¢–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
        return datetime.now()

scraper = NewsScraper()
