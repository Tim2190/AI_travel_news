import logging
import requests
import re
import urllib3
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from .config import settings

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger(__name__)

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞—Ç
MONTHS_RU = {
    "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4, "–º–∞—è": 5, "–∏—é–Ω—è": 6,
    "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8, "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12,
    "—è–Ω–≤": 1, "—Ñ–µ–≤": 2, "–º–∞—Ä": 3, "–∞–ø—Ä": 4, "–º–∞–π": 5, "–∏—é–Ω": 6,
    "–∏—é–ª": 7, "–∞–≤–≥": 8, "—Å–µ–Ω": 9, "–æ–∫—Ç": 10, "–Ω–æ—è": 11, "–¥–µ–∫": 12
}

MONTHS_KZ = {
    "“õ–∞“£—Ç–∞—Ä": 1, "–∞“õ–ø–∞–Ω": 2, "–Ω–∞—É—Ä—ã–∑": 3, "—Å”ô—É—ñ—Ä": 4, "–º–∞–º—ã—Ä": 5, "–º–∞—É—Å—ã–º": 6,
    "—à—ñ–ª–¥–µ": 7, "—Ç–∞–º—ã–∑": 8, "“õ—ã—Ä–∫“Ø–π–µ–∫": 9, "“õ–∞–∑–∞–Ω": 10, "“õ–∞—Ä–∞—à–∞": 11, "–∂–µ–ª—Ç–æ“õ—Å–∞–Ω": 12,
    "“õ–∞“£": 1, "–∞“õ–ø": 2, "–Ω–∞—É": 3, "—Å”ô—É": 4, "–º–∞–º": 5, "–º–∞—É": 6,
    "—à—ñ–ª": 7, "—Ç–∞–º": 8, "“õ—ã—Ä": 9, "“õ–∞–∑": 10, "“õ–∞—Ä": 11, "–∂–µ–ª": 12
}

# 1. –ü—Ä—è–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (Akorda, PrimeMinister)
DIRECT_SOURCES = [
    {
        "name": "Akorda (–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç)",
        "url": "https://www.akorda.kz/ru/events",
        "base_url": "https://www.akorda.kz",
        # –ò—â–µ–º –õ–Æ–ë–´–ï —Å—Å—ã–ª–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ /events/ –≤ –∞–¥—Ä–µ—Å–µ (—ç—Ç–æ –Ω–∞–¥–µ–∂–Ω–µ–µ –∫–ª–∞—Å—Å–æ–≤)
        "link_pattern": re.compile(r"/ru/events/[\w-]+"), 
        "container_tag": "div" # –û–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å –ø–æ–∏—Å–∫–∞ (–∏—â–µ–º –≤–Ω—É—Ç—Ä–∏ div)
    },
    {
        "name": "PrimeMinister (–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ)",
        "url": "https://primeminister.kz/ru/news",
        "base_url": "https://primeminister.kz",
        # –ò—â–µ–º –õ–Æ–ë–´–ï —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
        "link_pattern": re.compile(r"/ru/news/[\w-]+"),
        "container_tag": "div"
    }
]

# 2. GOV.KZ (API ID)
GOV_KZ_PROJECTS = {
    "–ú–∏–Ω–ù–∞—Ü–≠–∫–æ–Ω–æ–º–∏–∫–∏": 4, "–ú–∏–Ω–§–∏–Ω": 2, "–ú–ò–î –†–ö": 6, "–ú–í–î –†–ö": 11,
    "–ú–∏–Ω–¢—Ä—É–¥–∞": 21, "–ú–∏–Ω–ó–¥—Ä–∞–≤": 17, "–ú–∏–Ω–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è": 14, "–ú–∏–Ω–ù–∞—É–∫–∏": 15,
    "–ú–∏–Ω–ü—Ä–æ–º–°—Ç—Ä–æ–π": 3, "–ú–∏–Ω–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç": 22, "–ú–∏–Ω–¶–∏—Ñ—Ä—ã": 8, "–ú–∏–Ω–ö—É–ª—å—Ç—É—Ä—ã": 19,
    "–ú–∏–Ω–¢—É—Ä–∏–∑–º": 24, "–ú–∏–Ω–≠–∫–æ–ª–æ–≥–∏–∏": 16, "–ú–∏–Ω–°–µ–ª—å–•–æ–∑": 18, "–ú–∏–Ω–≠–Ω–µ—Ä–≥–æ": 20,
    "–ú–∏–Ω–Æ—Å—Ç": 9, "–ú–ß–° –†–ö": 5, "–ú–∏–Ω–¢–æ—Ä–≥–æ–≤–ª–∏": 23, "–ê–∫–∏–º–∞—Ç –ê–ª–º–∞—Ç—ã": 118, "–ê–∫–∏–º–∞—Ç –ê—Å—Ç–∞–Ω—ã": 105
}

class Scraper:
    def __init__(self):
        self.session = requests.Session()
        # –£–ü–†–û–©–ï–ù–ù–´–ï –ó–ê–ì–û–õ–û–í–ö–ò (—á—Ç–æ–±—ã –Ω–µ –ø—É–≥–∞—Ç—å —Å–µ—Ä–≤–µ—Ä 500-–π –æ—à–∏–±–∫–æ–π)
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        })

    def parse_date(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–µ—Ä –¥–∞—Ç—ã."""
        if not date_str: return None
        date_str = str(date_str).strip().lower()

        try:
            iso_clean = date_str.split("+")[0].split(".")[0].replace("z", "")
            if "t" in iso_clean: return datetime.fromisoformat(iso_clean)
            if len(iso_clean) == 10 and "-" in iso_clean: return datetime.strptime(iso_clean, "%Y-%m-%d")
        except: pass

        clean_text = re.sub(r"\s+\d{1,2}:\d{2}.*", "", date_str) 
        clean_text = re.sub(r"[^\w\s\.]", "", clean_text)
        
        if "." in clean_text:
            try: return datetime.strptime(clean_text, "%d.%m.%Y")
            except: pass

        parts = clean_text.split()
        if len(parts) >= 2:
            try:
                day = int(re.sub(r"\D", "", parts[0]))
                month_str = parts[1]
                month = MONTHS_RU.get(month_str) or MONTHS_KZ.get(month_str)
                year = datetime.now().year
                if len(parts) > 2 and parts[2].isdigit():
                    year = int(parts[2])
                    if 2020 < year < 2030: year = year
                if month: return datetime(year, month, day)
            except: pass
        return None

    def find_date_in_text(self, text: str) -> Optional[datetime]:
        """–ò—â–µ—Ç –¥–∞—Ç—É –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞."""
        if not text: return None
        head = text[:600]
        
        match_dots = re.search(r"\d{2}\.\d{2}\.\d{4}", head)
        if match_dots: return self.parse_date(match_dots.group(0))

        match_text = re.search(r"\d{1,2}\s+[–∞-—è–ê-–Ø”ô—ñ“£“ì“Ø“±“õ”©“ª]{3,}\s+\d{4}", head)
        if match_text: return self.parse_date(match_text.group(0))
        return None

    def scrape(self) -> List[Dict]:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞."""
        logger.warning("üèÅ STARTING SCRAPE CYCLE (SIMPLE MODE)...")
        all_news = []
        all_news.extend(self.scrape_gov_kz_api())
        for source in DIRECT_SOURCES:
            all_news.extend(self.scrape_direct(source))
        logger.warning(f"‚úÖ CYCLE FINISHED. Total items found: {len(all_news)}")
        return all_news

    def scrape_gov_kz_api(self) -> List[Dict]:
        results = []
        base_api = "https://gov.kz/api/v1/public/news"
        
        for name, project_id in GOV_KZ_PROJECTS.items():
            try:
                params = {"projects": project_id, "lang": "ru", "limit": 3}
                # –¢–∞–π–º–∞—É—Ç 10 —Å–µ–∫, –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                resp = self.session.get(base_api, params=params, timeout=10, verify=False)
                
                if resp.status_code == 200:
                    data = resp.json()
                    items = data if isinstance(data, list) else data.get("content", [])
                    
                    for item in items:
                        title = item.get("title")
                        if not title: continue

                        pub_date = self.parse_date(item.get("publish_date") or item.get("created_date"))
                        body = item.get("body") or ""
                        soup = BeautifulSoup(body, "html.parser")
                        text = soup.get_text(separator="\n").strip()

                        if not pub_date: pub_date = self.find_date_in_text(text)
                        # FALLBACK: –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî —Å—Ç–∞–≤–∏–º –°–ï–ô–ß–ê–°
                        if not pub_date: pub_date = datetime.now()

                        news_id = item.get("id")
                        proj_id_from_api = item.get("projects", [project_id])[0]
                        link = f"https://gov.kz/memleket/entities/{proj_id_from_api}/press/news/details/{news_id}?lang=ru"
                        
                        img = None
                        if item.get("visual_content"):
                             img = item["visual_content"][0].get("source")

                        results.append({
                            "title": title, "original_text": text[:4000],
                            "source_name": name, "source_url": link,
                            "published_at": pub_date, "image_url": img
                        })
                    if items: logger.info(f"API {name}: found {len(items)}")
                else:
                    logger.warning(f"API {name} Error: Status {resp.status_code}")
            except Exception as e:
                logger.error(f"API {name} Exception: {e}")
        return results

    def scrape_direct(self, config: Dict) -> List[Dict]:
        results = []
        name = config["name"]
        try:
            resp = self.session.get(config["url"], timeout=20, verify=False)
            if resp.status_code != 200: 
                logger.warning(f"Direct {name}: Status {resp.status_code}")
                return []
            
            soup = BeautifulSoup(resp.content, "html.parser")
            
            # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ <a>, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥ –ø–∞—Ç—Ç–µ—Ä–Ω
            # –≠—Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å—ã –∏ –∏—â–µ—Ç –ø—Ä–æ—Å—Ç–æ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ URL
            seen_links = set()
            found_links = []
            
            for a in soup.find_all("a", href=True):
                href = a["href"]
                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ –ø–∞—Ç—Ç–µ—Ä–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä /ru/events/...)
                if config["link_pattern"].search(href):
                    full_link = config["base_url"] + href if href.startswith("/") else href
                    if full_link not in seen_links:
                        seen_links.add(full_link)
                        found_links.append((a, full_link))
                        if len(found_links) >= 3: break # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 3 —Å–≤–µ–∂–∏—Ö

            if not found_links:
                logger.warning(f"Direct {name}: No matching links found")

            for link_el, full_link in found_links:
                title = link_el.get_text(strip=True)
                if len(title) < 5: continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º—É—Å–æ—Ä

                full_text, image, pub_date = self.fetch_details(full_link)
                
                if not pub_date: pub_date = datetime.now()
                
                results.append({
                    "title": title, "original_text": full_text or title,
                    "source_name": name, "source_url": full_link,
                    "published_at": pub_date, "image_url": image
                })
            
            if results: logger.info(f"Direct {name}: found {len(results)}")
        except Exception as e:
            logger.error(f"Direct {name} Error: {e}")
        return results

    def fetch_details(self, url: str):
        try:
            resp = self.session.get(url, timeout=10, verify=False)
            soup = BeautifulSoup(resp.content, "html.parser")
            text = soup.get_text(separator="\n").strip()
            
            img = None
            meta_img = soup.find("meta", property="og:image")
            if meta_img: img = meta_img.get("content")

            pub_date = None
            for prop in ["article:published_time", "published_time", "date"]:
                meta = soup.find("meta", property=prop) or soup.find("meta", attrs={"name": prop})
                if meta:
                    pub_date = self.parse_date(meta.get("content"))
                    if pub_date: break
            
            if not pub_date: pub_date = self.find_date_in_text(text)

            return text, img, pub_date
        except:
            return None, None, None

scraper = Scraper()
