import asyncio
import html
import logging
import re
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import func
from difflib import SequenceMatcher
from .database import SessionLocal, NewsArchive, NewsStatus
from .scraper import scraper
from .rewriter import rewriter
from .publisher import publisher
from .config import settings
import requests

logger = logging.getLogger(__name__)

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def is_fuzzy_duplicate(new_title: str, existing_titles: list, threshold=0.65) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂ –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –æ–¥–∏–Ω –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö."""
    if not new_title:
        return False
    new_lower = new_title.lower()
    for old_title in existing_titles:
        if not old_title:
            continue
        similarity = SequenceMatcher(None, new_lower, old_title.lower()).ratio()
        if similarity > threshold:
            return True
    return False

def is_text_kazakh(text: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞."""
    if not text: return False
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∑–∞—Ö—Å–∫–æ–≥–æ –∞–ª—Ñ–∞–≤–∏—Ç–∞
    kz_chars = r'[”ô—ñ“£“ì“Ø“±“õ”©“ª”ò–Ü“¢“í“Æ“∞“ö”®“∫]'
    return bool(re.search(kz_chars, text))

# --- –û–°–ù–û–í–ù–´–ï –ó–ê–î–ê–ß–ò ---

async def scrape_news_task():
    """
    Scrape news, select up to 5 best items and store as drafts.
    """
    db = SessionLocal()
    try:
        logger.info("Starting scraping cycle...")
        new_items = scraper.scrape()
        if not new_items:
            logger.warning("No news found from any direct sources.")
            return

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        topic_keywords = [k.strip().lower() for k in settings.TOPIC_KEYWORDS.split(",") if k.strip()]
        def matches_topic(item):
            if not topic_keywords:
                return True
            title = (item.get("title") or "").lower()
            text = (item.get("original_text") or "").lower()
            combined = f"{title} {text}"
            return any(kw in combined for kw in topic_keywords)
        
        new_items = [i for i in new_items if matches_topic(i)]
        if not new_items:
            logger.warning("No news matching topic keywords.")
            return

        # --- –ñ–ï–°–¢–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –ê–ö–¢–£–ê–õ–¨–ù–û–°–¢–ò ---
        cutoff = datetime.utcnow() - timedelta(days=settings.NEWS_MAX_AGE_DAYS)
        def is_recent(item):
            pub = item.get("published_at")
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî —Å—á–∏—Ç–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∏ –ù–ï –±–µ—Ä–µ–º
            if pub is None:
                logger.warning(f"Rejected (no date): {item.get('title', 'Unknown')[:50]}...")
                return False
                
            if getattr(pub, "tzinfo", None):
                pub = pub.replace(tzinfo=None)
            
            check_ok = pub >= cutoff
            if not check_ok:
                logger.info(f"Skipped (outdated, from {pub}): {item.get('title')[:50]}...")
            return check_ok
        
        new_items = [i for i in new_items if is_recent(i)]
        if not new_items:
            logger.warning("No recent news found after filtering dates.")
            return

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Å–∫–æ—Ä–∏–Ω–≥ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤–∞–∂–Ω—ã–º —Ç–µ–º–∞–º)
        def normalize_title(title):
            if not title: return ""
            return re.sub(r"\s+", " ", title.strip().lower())[:500]

        def score(item):
            text = (item.get("original_text") or "").lower()
            title = (item.get("title") or "").lower()
            base = min(len(text) / 500, 3) 
            keywords = ["—ç–∫–æ–Ω–æ–º–∏–∫–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–∑–∞–∫–æ–Ω", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ"]
            kw_score = sum(2 for k in keywords if k in text or k in title)
            return base + kw_score

        scored = sorted(new_items, key=score, reverse=True)
        top_items = scored[:10] # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–µ–π

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤
        check_date = datetime.utcnow() - timedelta(days=3)
        recent_records = db.query(NewsArchive.title).filter(NewsArchive.created_at >= check_date).all()
        existing_titles_cache = [row[0] for row in recent_records if row[0]]

        added_count = 0
        for item in top_items:
            if added_count >= 5: break # –õ–∏–º–∏—Ç –Ω–∞ –æ–¥–∏–Ω —Ü–∏–∫–ª —Å–±–æ—Ä–∞

            current_title = item["title"]
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ URL
            if db.query(NewsArchive).filter(NewsArchive.source_url == item["source_url"]).first():
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –∑–∞–≥–æ–ª–æ–≤–∫—É
            norm = normalize_title(current_title)
            if norm and db.query(NewsArchive).filter(NewsArchive.normalized_title == norm).first():
                continue

            # Fuzzy matching
            if is_fuzzy_duplicate(current_title, existing_titles_cache, threshold=0.65):
                logger.info(f"Skipping fuzzy duplicate: '{current_title}'")
                continue

            news_entry = NewsArchive(
                title=current_title,
                normalized_title=norm or None,
                original_text=item["original_text"],
                source_name=item["source_name"],
                source_url=item["source_url"],
                source_published_at=item.get("published_at"),
                image_url=item["image_url"],
                status=NewsStatus.draft.value
            )
            db.add(news_entry)
            added_count += 1
            existing_titles_cache.append(current_title)

        db.commit()
        logger.info(f"Successfully added {added_count} prioritized news items to drafts.")
    except Exception as e:
        logger.error(f"Error in scrape_news_task: {str(e)}")
    finally:
        db.close()

async def process_news_task():
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—á–µ—Ä–µ–¥–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π. 
    –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–æ—Ç –±–æ–ª—å—à–µ –Ω–µ –∑–∞–≤–∏—Å–∞–µ—Ç, –µ—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —è–∑—ã–∫–µ.
    """
    db = SessionLocal()
    try:
        logger.info("Starting news processing cycle (with limits)...")

        # 1. –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê –°–ï–ì–û–î–ù–Ø
        today_start = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
        published_today = db.query(NewsArchive).filter(
            NewsArchive.status == NewsStatus.published.value,
            NewsArchive.published_at >= today_start
        ).all()

        kz_count = sum(1 for p in published_today if is_text_kazakh(p.rewritten_text or p.title))
        ru_count = len(published_today) - kz_count

        # –ò—â–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π KZ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        last_kz_pub_time = datetime.min
        for p in published_today:
            if is_text_kazakh(p.rewritten_text or p.title):
                if p.published_at and p.published_at > last_kz_pub_time:
                    last_kz_pub_time = p.published_at

        logger.info(f"Today's stats: KZ {kz_count}/20, RU {ru_count}/40")

        # 2. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ò–û–†–ò–¢–ï–¢–ù–û–ì–û –Ø–ó–´–ö–ê
        target_lang = "RU"
        time_since_kz = datetime.utcnow() - last_kz_pub_time
        
        # –ï—Å–ª–∏ –ø—Ä–æ—à–µ–ª —á–∞—Å –∏ –ª–∏–º–∏—Ç KZ –Ω–µ –∏—Å—á–µ—Ä–ø–∞–Ω ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç KZ
        if kz_count < 20 and time_since_kz >= timedelta(hours=1):
            target_lang = "KZ"
            logger.info("Priority target: Kazakh (Hour interval reached)")

        # 3. –ü–û–ò–°–ö –ß–ï–†–ù–û–í–ò–ö–ê
        drafts = db.query(NewsArchive).filter(
            NewsArchive.status == NewsStatus.draft.value
        ).order_by(NewsArchive.created_at.asc()).all()

        if not drafts:
            logger.info("No drafts available in database.")
            return

        selected = None
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ
        for d in drafts:
            is_kz = is_text_kazakh(d.original_text)
            if target_lang == "KZ" and is_kz:
                selected = d
                break
            if target_lang == "RU" and not is_kz:
                selected = d
                break
        
        # FALLBACK: –ï—Å–ª–∏ –Ω–∞ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ –Ω–∏—á–µ–≥–æ –Ω–µ—Ç, –±–µ—Ä–µ–º –ü–ï–†–í–£–Æ –õ–Æ–ë–£–Æ –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        if not selected:
            logger.info(f"No drafts found for {target_lang}. Taking first available draft to avoid idle time.")
            selected = drafts[0]

        # 4. –†–ï–†–ê–ô–¢ –ò –ü–£–ë–õ–ò–ö–ê–¶–ò–Ø
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç –∏–∑ –±–∞–∑—ã, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å —Å–µ—Å—Å–∏–µ–π
            selected = db.merge(selected)
            logger.info(f"--- Processing: {selected.title} ---")
            
            rewritten = await rewriter.rewrite(selected.original_text)
            if not rewritten:
                selected.status = NewsStatus.error.value
                db.commit()
                return
            
            # –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï –ò –î–û–ë–ê–í–õ–ï–ù–ò–ï –î–ò–°–ö–õ–ï–ô–ú–ï–†–ê
            safe_url = html.escape(selected.source_url, quote=True)
            disclaimer = "\n\n<i>‚ö†Ô∏è –°–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ –ò–ò. –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Å—Å—ã–ª–∫–µ –Ω–∏–∂–µ.</i>"
            source_link = f"\n\n<a href=\"{safe_url}\">üåê –¢“Ø–ø–Ω“±—Å“õ–∞ / –ò—Å—Ç–æ—á–Ω–∏–∫</a>"
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            final_text = f"{rewritten}{disclaimer}{source_link}"
            
            # –ü—É–±–ª–∏–∫–∞—Ü–∏—è
            post_id = await publisher.publish(final_text, selected.image_url)
            
            if post_id:
                selected.telegram_post_id = str(post_id)
                selected.status = NewsStatus.published.value
                selected.published_at = datetime.utcnow()
                selected.rewritten_text = rewritten
                db.commit()
                logger.info(f"Successfully published ID {selected.id}. Post ID: {post_id}")
            else:
                raise Exception("Publisher returned empty post_id")
            
        except Exception as e:
            logger.error(f"Error in publishing {selected.id}: {str(e)}")
            selected.status = NewsStatus.error.value
            db.commit()

    except Exception as e:
        logger.error(f"Error in process_news_task: {str(e)}")
    finally:
        db.close()

def is_post_integrity_ok(final_text: str, source_url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –ø–µ—Ä–µ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π."""
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ—Ç—É
    if not final_text or len(final_text) < 150:
        logger.error("integrity Check Failed: –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π.")
        return False
        
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–∫–∏
    if not source_url or "http" not in source_url:
        logger.error("Integrity Check Failed: –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫.")
        return False
        
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ)
    if "<b>" not in final_text:
        logger.error("Integrity Check Failed: –í —Ç–µ–∫—Å—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—Ç–µ–≥ <b>).")
        return False
        
    if "–¢“Ø–ø–Ω“±—Å“õ–∞" not in final_text and "–ò—Å—Ç–æ—á–Ω–∏–∫" not in final_text:
        logger.error("Integrity Check Failed: –í —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª.")
        return False

    return True
    
def start_scheduler():
    from apscheduler.schedulers.asyncio import AsyncIOScheduler

    scheduler = AsyncIOScheduler()
    
    # –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π (–∏–Ω—Ç–µ—Ä–≤–∞–ª –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä 20 –º–∏–Ω)
    scheduler.add_job(scrape_news_task, 'interval', minutes=settings.SCRAPE_INTERVAL_MINUTES)
    
    # –ü—É–±–ª–∏–∫–∞—Ü–∏—è (–∏–Ω—Ç–µ—Ä–≤–∞–ª –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä 5 –∏–ª–∏ 15 –º–∏–Ω)
    scheduler.add_job(process_news_task, 'interval', minutes=settings.PUBLISH_INTERVAL_MINUTES)
    
    # –ü–∏–Ω–≥ —Å–∞–º–æ–≥–æ —Å–µ–±—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Å–Ω–∞ –Ω–∞ Koyeb
    def ping_self():
        try:
            requests.get("http://127.0.0.1:8000/health", timeout=5)
            logger.info("Keepalive ping OK")
        except Exception as e:
            logger.warning(f"Keepalive ping failed: {e}")
            
    scheduler.add_job(ping_self, 'interval', minutes=4)
    
    scheduler.start()
    logger.info("APScheduler started successfully.")
    return scheduler
