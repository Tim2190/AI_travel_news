import asyncio
import html
import logging
import re
from datetime import datetime, time, timedelta
from difflib import SequenceMatcher
from sqlalchemy.orm import Session
from sqlalchemy import func
import requests
import pytz # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —á–∞—Å–æ–≤—ã–º –ø–æ—è—Å–æ–º –ê—Å—Ç–∞–Ω—ã

from .database import SessionLocal, NewsArchive, NewsStatus
from .scraper import scraper
from .rewriter import rewriter
from .publisher import publisher
from .config import settings

logger = logging.getLogger(__name__)

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
TIMEZONE = pytz.timezone('Asia/Almaty')
WORK_START = time(7, 0)  # 07:00 —É—Ç—Ä–∞
WORK_END = time(21, 0)   # 21:00 –≤–µ—á–µ—Ä–∞

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def is_fuzzy_duplicate(new_title: str, existing_titles: list, threshold=0.65) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂ –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –æ–¥–∏–Ω –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö."""
    if not new_title: return False
    new_lower = new_title.lower()
    for old_title in existing_titles:
        if not old_title: continue
        similarity = SequenceMatcher(None, new_lower, old_title.lower()).ratio()
        if similarity > threshold:
            return True
    return False

def is_text_kazakh(text: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞."""
    if not text: return False
    kz_chars = r'[”ô—ñ“£“ì“Ø“±“õ”©“ª”ò–Ü“¢“í“Æ“∞“ö”®“∫]'
    return bool(re.search(kz_chars, text, re.IGNORECASE))

def is_post_integrity_ok(final_text: str, source_url: str) -> bool:
    """–ö–û–ù–¢–†–û–õ–ï–†: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å—Ç–∞ –ø–µ—Ä–µ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π."""
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ—Ç—É –∏ –¥–ª–∏–Ω—É
    if not final_text or len(final_text) < 100:
        logger.error("‚ùå Integrity Check: –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π.")
        return False
        
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Ç–µ–∫—Å—Ç–µ
    if "–¢“Ø–ø–Ω“±—Å“õ–∞" not in final_text and "–ò—Å—Ç–æ—á–Ω–∏–∫" not in final_text:
        logger.error("‚ùå Integrity Check: –°—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ.")
        return False
        
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–∑–∞–≥–æ–ª–æ–≤–æ–∫)
    if "<b>" not in final_text:
        logger.error("‚ùå Integrity Check: –ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ (—Ç–µ–≥ <b>).")
        return False

    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∞–º–æ–≥–æ URL
    if not source_url or "http" not in source_url:
        logger.error("‚ùå Integrity Check: –ë–∏—Ç—ã–π URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞.")
        return False

    return True

# --- –ó–ê–î–ê–ß–ò ---

async def scrape_news_task():
    """–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∂–µ—Å—Ç–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –¥–∞—Ç."""
    db = SessionLocal()
    try:
        logger.info("Starting scraping cycle...")
        new_items = scraper.scrape()
        if not new_items:
            logger.warning("No news found from direct sources.")
            return

        # 1. –§–∏–ª—å—Ç—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        topic_keywords = [k.strip().lower() for k in settings.TOPIC_KEYWORDS.split(",") if k.strip()]
        def matches_topic(item):
            if not topic_keywords: return True
            text_blob = (f"{item.get('title', '')} {item.get('original_text', '')}").lower()
            return any(kw in text_blob for kw in topic_keywords)
        
        new_items = [i for i in new_items if matches_topic(i)]

        # 2. –ñ–ï–°–¢–ö–ò–ô –§–ò–õ–¨–¢–† –ü–û –î–ê–¢–ï (–¢–æ–ª—å–∫–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏)
        cutoff = datetime.utcnow() - timedelta(days=settings.NEWS_MAX_AGE_DAYS)
        def is_recent(item):
            pub = item.get("published_at")
            if not pub: # –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –≤ –º—É—Å–æ—Ä–∫—É
                return False
            if getattr(pub, "tzinfo", None):
                pub = pub.replace(tzinfo=None)
            return pub >= cutoff
        
        new_items = [i for i in new_items if is_recent(i)]
        
        if not new_items:
            logger.info("No recent news found after filtering.")
            return

        # 3. –°–∫–æ—Ä–∏–Ω–≥ –∏ –æ—Ç–±–æ—Ä
        new_items.sort(key=lambda x: len(x.get('original_text', '')), reverse=True)
        top_items = new_items[:10]

        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–µ–π)
        check_date = datetime.utcnow() - timedelta(days=3)
        recent_titles = [r[0] for r in db.query(NewsArchive.title).filter(NewsArchive.created_at >= check_date).all()]
        
        added = 0
        for item in top_items:
            if added >= 5: break
            
            title = item["title"]
            if db.query(NewsArchive).filter(NewsArchive.source_url == item["source_url"]).first():
                continue
            if is_fuzzy_duplicate(title, recent_titles):
                continue

            db.add(NewsArchive(
                title=title,
                original_text=item["original_text"],
                source_name=item["source_name"],
                source_url=item["source_url"],
                source_published_at=item.get("published_at"),
                image_url=item["image_url"],
                status=NewsStatus.draft.value
            ))
            added += 1
            recent_titles.append(title)
        
        db.commit()
        logger.info(f"Added {added} new drafts.")
        
    except Exception as e:
        logger.error(f"Scrape Error: {e}")
    finally:
        db.close()

async def process_news_task():
    """–ü—É–±–ª–∏–∫–∞—Ü–∏—è: –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã 07-21, –ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ 2 RU / 1 KZ."""
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—á–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (–ê—Å—Ç–∞–Ω–∞)
    now_kz = datetime.now(TIMEZONE).time()
    if not (WORK_START <= now_kz <= WORK_END):
        logger.info(f"üò¥ Zzz... Time is {now_kz.strftime('%H:%M')}. Working hours: 07:00-21:00.")
        return

    db = SessionLocal()
    try:
        logger.info("Starting processing cycle...")

        # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥–∏ (2 RU -> 1 KZ)
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        last_posts = db.query(NewsArchive).filter(
            NewsArchive.status == NewsStatus.published.value
        ).order_by(NewsArchive.published_at.desc()).limit(3).all()

        target_lang = "RU" # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        if last_posts:
            # –õ–æ–≥–∏–∫–∞ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏—è:
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è –±—ã–ª–∞ KZ -> –°–µ–π—á–∞—Å RU
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è –±—ã–ª–∞ RU, –∏ –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω—è—è RU -> –°–µ–π—á–∞—Å KZ
            p1 = last_posts[0] # –°–∞–º–∞—è —Å–≤–µ–∂–∞—è
            
            p1_is_kz = is_text_kazakh(p1.rewritten_text or p1.title)
            
            if p1_is_kz:
                target_lang = "RU"
                logger.info("Rotation: Last was KZ -> Next RU")
            else:
                # –ü–æ—Å–ª–µ–¥–Ω—è—è –±—ã–ª–∞ RU. –°–º–æ—Ç—Ä–∏–º –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω—é—é.
                if len(last_posts) >= 2:
                    p2 = last_posts[1]
                    p2_is_kz = is_text_kazakh(p2.rewritten_text or p2.title)
                    if not p2_is_kz: # –ò –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω—è—è —Ç–æ–∂–µ –Ω–µ KZ (–∑–Ω–∞—á–∏—Ç –±—ã–ª–æ 2 RU –ø–æ–¥—Ä—è–¥)
                        target_lang = "KZ"
                        logger.info("Rotation: Last 2 were RU -> Next KZ")
                    else:
                        target_lang = "RU" # –ë—ã–ª–æ RU, KZ -> –ó–Ω–∞—á–∏—Ç –µ—â–µ –æ–¥–Ω–æ RU
                else:
                    target_lang = "RU" # –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞ –≥–æ–Ω–∏–º RU

        # 3. –ü–æ–∏—Å–∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —á–µ—Ä–Ω–æ–≤–∏–∫–∞
        drafts = db.query(NewsArchive).filter(NewsArchive.status == NewsStatus.draft.value).all()
        if not drafts:
            logger.info("No drafts.")
            return

        selected = None
        # –ò—â–µ–º —Å—Ç—Ä–æ–≥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —è–∑—ã–∫–∞
        for d in drafts:
            draft_is_kz = is_text_kazakh(d.original_text)
            if target_lang == "KZ" and draft_is_kz:
                selected = d
                break
            if target_lang == "RU" and not draft_is_kz:
                selected = d
                break
        
        # Fallback: –ï—Å–ª–∏ –Ω—É–∂–Ω–æ–≥–æ —è–∑—ã–∫–∞ –Ω–µ—Ç, –±–µ—Ä–µ–º —á—Ç–æ –µ—Å—Ç—å (—á—Ç–æ–±—ã –Ω–µ —Å—Ç–æ—è—Ç—å)
        if not selected:
            selected = drafts[0]
            logger.info(f"Fallback: No {target_lang} drafts. Taking available.")

        # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞
        try:
            selected = db.merge(selected)
            logger.info(f"Processing: {selected.title}...")

            # –†–µ—Ä–∞–π—Ç —á–µ—Ä–µ–∑ Gemini Ensemble
            rewritten = await rewriter.rewrite(selected.original_text)
            
            if not rewritten:
                selected.status = NewsStatus.error.value
                db.commit()
                return

            # –°–±–æ—Ä–∫–∞
            safe_url = html.escape(selected.source_url, quote=True)
            disclaimer = "\n\n<i>‚ö†Ô∏è –°–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ –ò–ò. –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Å—Å—ã–ª–∫–µ –Ω–∏–∂–µ.</i>"
            source_link = f"\n<a href=\"{safe_url}\">üåê –¢“Ø–ø–Ω“±—Å“õ–∞ / –ò—Å—Ç–æ—á–Ω–∏–∫</a>"
            final_text = f"{rewritten}{disclaimer}{source_link}"

            # –ö–æ–Ω—Ç—Ä–æ–ª—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
            if not is_post_integrity_ok(final_text, selected.source_url):
                logger.warning(f"‚ö†Ô∏è Rejected by Integrity Check: {selected.id}")
                selected.status = NewsStatus.error.value
                db.commit()
                return

            # –ü—É–±–ª–∏–∫–∞—Ü–∏—è
            post_id = await publisher.publish(final_text, selected.image_url)
            
            if post_id:
                selected.telegram_post_id = str(post_id)
                selected.status = NewsStatus.published.value
                selected.published_at = datetime.utcnow()
                selected.rewritten_text = rewritten
                db.commit()
                logger.info(f"‚úÖ Published: {post_id}")
            
        except Exception as e:
            logger.error(f"Processing Error: {e}")
            selected.status = NewsStatus.error.value
            db.commit()

    except Exception as e:
        logger.error(f"Task Error: {e}")
    finally:
        db.close()

def start_scheduler():
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
    scheduler = AsyncIOScheduler()
    scheduler.add_job(scrape_news_task, 'interval', minutes=settings.SCRAPE_INTERVAL_MINUTES)
    scheduler.add_job(process_news_task, 'interval', minutes=settings.PUBLISH_INTERVAL_MINUTES)
    
    def ping():
        try: requests.get("http://127.0.0.1:8000/health", timeout=5)
        except: pass
    scheduler.add_job(ping, 'interval', minutes=4)
    
    scheduler.start()
    return scheduler
